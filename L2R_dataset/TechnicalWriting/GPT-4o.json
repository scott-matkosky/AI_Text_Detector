["Process calculi grounded in logic, like $\\pi$DILL and CP, provide a robust basis for deadlock-free concurrent programming. However, prior research reveals a discrepancy between the proof-construction rules and the term constructors of the $\\pi$-calculus: specifically, the primary operator for parallel composition does not align with any rule in linear logic. Kokke et al. (2019) proposed Hypersequent Classical Processes (HCP) to resolve this inconsistency by employing hypersequents (collections of sequents) to encapsulate parallelism in the typing judgments. Despite this innovation, the leap from CP to HCP is substantial. Currently, HCP lacks reduction semantics, and the introduction of delayed actions causes CP processes interpreted as HCP processes to behave differently than in CP. We present HCP-, a variant of HCP that includes reduction semantics and eliminates delayed actions. We demonstrate progress, preservation, and termination, and show that HCP- upholds the same communication protocols as CP.", "This text presents a variant of the BDDC preconditioner that imposes constraints on selected subdomain subobjects (subedges, subfaces, and vertices between pairs of subedges). We show that the preconditioner's condition number is bounded by $C \\big(1+\\log (L/h)\\big)^2$, where $C$ is a constant, and $h$ and $L$ are the characteristic sizes of the mesh and the subobjects, respectively. By choosing $L$ appropriately, the condition number can be as small as $O(1)$. We discuss the preconditioner's advantages and disadvantages and its application to heterogeneous problems, providing numerical results on supercomputers.", "We provide instances where the Heun function appears as solutions to wave equations found in general relativity. In four-dimensional spacetime, the Dirac equation within the context of the Nutku helicoid metric produces Mathieu functions as its solutions. When extended to five dimensions, this scenario leads to the double confluent Heun function. Through specific transformations, we simplify this solution back to the Mathieu function. Due to the singularity at the origin within this metric, it is necessary to implement Atiyah-Patodi-Singer spectral boundary conditions for the system.", "Many authors have shown that the gradual decrease in X-rays during the decay phase of long-duration flares (LDEs) can only be explained by continuous magnetic reconnection and energy release in the flare's coronal region. Using RHESSI data, we aim to determine the effectiveness of these processes during the LDE decay phase and how accurately the energy release rate can be calculated. To achieve this, we reconstructed images of selected LDEs during their decay phase. The physical parameters of flare coronal sources from image spectral analysis helped us evaluate the energy release efficiency. We also examined the terms in the energy equation to assess the accuracy of each term's determination.", "Using a multi-scale analysis, we characterize the typical geometric structure of clusters under the FK measure in random media. Our results are valid in dimensions of 2 or higher, assuming slab percolation occurs under the averaged measure, which is expected throughout the supercritical phase. This work builds on Pisztora's findings and offers a crucial tool for analyzing the supercritical regime in disordered FK models and the related disordered Ising and Potts models.", "Photospheric absorption lines in classical T Tauri stars (CTTS) are pretty weak compared to normal stars. This phenomenon, called veiling, usually happens because of extra continuous emission from shock-heated gas at the stellar surface below the accretion streams. We picked four stars (RW Aur A, RU Lup, S CrA NW, and S CrA SE) with notably strong veiling to look closely at how veiling relates to stellar brightness and emission line strengths compared to standard accretion models. We've monitored these stars both photometrically and spectroscopically at various times.\n\nIn typical accretion models, if the accretion rate changes, the excess emission should change too. So, the star's brightness should vary as well. We found that the veiling of absorption lines in these stars changes a lot and is usually so strong that it would need the release of several stellar luminosities of potential energy. During states of very high line dilution, the large veiling factors derived only weakly correlate with brightness. Plus, the emission line strengths don't follow the expected trend of veiling versus line strength. Veiling can change dramatically in just one night and isn't linked to the rotation periods found for two of the stars.\n\nIn at least three of the stars, when veiling gets high, the photospheric lines get filled in by line emission, leading to large veiling factors that aren't connected to any changes in continuous emission from shock regions. We also look into how much dust extinction and electron scattering in the accretion stream might affect veiling measurements in CTTS. In the end, we conclude that the degree of veiling can't be used as a measure of accretion rates in CTTS with a lot of emission lines.", "Giant low surface brightness (GLSB) galaxies are typically believed to be massive systems dominated by dark matter, based on uncertain rotation curve data. In this study, we examine two prototypical GLSB galaxies: Malin 1 and NGC 7589. By re-analyzing existing HI observations, we derived new rotation curves to investigate the distributions of luminous and dark matter within these galaxies. Contrary to previous studies, our rotation curves reveal a steep rise in the central regions, characteristic of high surface brightness (HSB) systems. Mass decomposition with a dark matter halo indicates that baryons might dominate the dynamics in these inner regions. A \"maximum disk\" fit yields stellar mass-to-light ratios consistent with those typically found in HSB galaxies. These findings, along with other recent studies, suggest that GLSB galaxies have a double structure: an inner HSB early-type spiral galaxy and an extended outer LSB disk. Additionally, we tested the Modified Newtonian Dynamics (MOND) theory: it successfully reproduces the rotation curve of NGC 7589, while Malin 1 presents a more challenging case for the theory.", "This study investigates the multiplicity distribution, multiplicity moments, scaled variance, entropy, and reduced entropy of target-evaporated fragments emitted in forward and backward hemispheres during interactions between emulsion heavy targets (AgBr) and projectiles such as 12 A GeV $^{4}$He, 3.7 A GeV $^{16}$O, 60 A GeV $^{16}$O, 1.7 A GeV $^{84}$Kr, and 10.7 A GeV $^{197}$Au. The findings reveal that the multiplicity distribution of these target-evaporated fragments follows a Gaussian distribution in both hemispheres. Additionally, the multiplicity moments of these fragments show an increase with the order of the moment (q). Notably, the second-order multiplicity moment remains consistent across all energy levels for each interaction type in both hemispheres. The scaled variance, which measures multiplicity fluctuations, is approximately one for all interactions, indicating a weak correlation among the produced particles. Furthermore, the entropy of the target-evaporated fragments is found to be the same within experimental errors for both hemispheres.", "We theoretically investigate the temporal behavior of a quantum dot under off-resonant optical excitation aimed at fast acoustic phonon-assisted state preparation. We show that short laser pulses drive three key processes: state dressing during pulse onset, phonon-induced relaxation, and state undressing at pulse end. By analyzing different pulse shapes, we emphasize the critical role of adiabatic undressing on the final state. Additionally, in exciton-biexciton systems, the laser characteristics\u2014pulse detuning, pulse length, and biexciton binding energy\u2014enable the selection of the targeted quantum dot state.", "In quantum mechanics, the probabilistic interpretation is an additional aspect imposed by experimental evidence, not inherent in the mathematical model. However, quantum logics with unique conditional probabilities provide a model with a clear probabilistic interpretation from the start. This includes the projection lattices in von Neumann algebras, where probability updates align with the state transition during the Lueders-von Neumann measurement process. This framework defines five levels of compatibility and measurability in quantum logics with unique conditional probabilities: absence of quantum interference, existence of a joint distribution, simultaneous measurability, independence from the measurement order, and belonging to the same Boolean subalgebra. Generally, these five levels differ, but they coincide in the common Hilbert space formalism of quantum mechanics, von Neumann algebras, and some other cases.", "This study analyzes wave-vector dispersion in elliptically birefringent stratified magneto-optic media with one-dimensional periodicity. It reveals that differences in local normal-mode polarization states between adjacent layers cause mode coupling, which affects wave-vector dispersion and the nature of the system's Bloch states. This mode coupling introduces additional terms into the dispersion relation that are absent in uniformly circularly birefringent magneto-optic stratified media. Under certain conditions, normal mode coupling eliminates degeneracy at frequency band crossover points and creates a magnetization-dependent optical band gap. The research investigates the conditions necessary for band gap formation and demonstrates that the frequency split can be characterized by a simple coupling parameter based on the polarization state relationships of adjacent layers' local normal modes. It also analyzes the characteristics of Bloch states and the conditions for optimizing band splitting strength in these systems.", "We delve into a broader version of traditional empirical risk minimization, where the hypothesis space emerges as a random subspace within an established space. Specifically, we explore subspaces that might be influenced by data, forming from a random selection of the data, which includes the Nystr\u00f6m methods for kernel techniques as a specific example. Utilizing random subspaces can naturally cut down on computational expenses, however, it raises the question of whether this approach compromises learning accuracy. These trade-offs between statistics and computation have been recently investigated for loss functions like least squares and self-concordant types, such as the logistic loss. Here, we aim to broaden these findings to encompass convex Lipschitz loss functions, which may not exhibit smoothness, like the hinge loss employed in support vector machines. Achieving this extension necessitates crafting new proofs with distinct technical approaches. Our principal findings indicate that there are various scenarios, contingent upon the complexity of the learning task, where enhancing computational efficiency does not result in a performance drop. These theoretical insights are backed by straightforward numerical experiments.", "The concept of patient consent is crucial for accessing medical data. In traditional healthcare systems, consent is obtained through a form that the patient completes and signs. In e-Health systems, this paper-form consent is being replaced by embedding consent mechanisms directly into the systems that control access to medical data. This transition empowers patients to grant and revoke consent more efficiently. However, the procedures for granting and revoking consent can vary significantly depending on the patient's circumstances. Our central argument is that capturing such detailed consent requirements through a set of authorization policies is highly complex and prone to errors. In this paper, we introduce ACTORS, a goal-driven method for managing consent. The key idea behind ACTORS is to utilize the goal-oriented approach of Teleo-Reactive (TR) programming to manage consent, accommodating changes in the domains and contexts in which patients provide their consent.", "This paper analyzes the inverse random source problem for the time fractional diffusion equation driven by fractional Brownian motion. It examines the stochastic time fractional diffusion equation (direct problem) and aims to determine the statistical properties of the source from final time data (inverse problem). The direct problem is shown to be well-posed with a unique mild solution under certain conditions. For the inverse problem, uniqueness is proved and instability is characterized, utilizing the Mittag\u2013Leffler function and stochastic integrals associated with fractional Brownian motion.", "Dive into the world of manifold learning, where advanced methods revolutionize nonlinear dimensionality reduction, tackling high-dimensional data sets with a low intrinsic dimensionality. Central to these methods are graph-based techniques: each data point is represented by a vertex, and weighted edges define connections between pairs. Traditional theories have shown that the graph's Laplacian matrix approximates the Laplace-Beltrami operator on the data manifold, assuming Euclidean-based affinities.\n\nIn an exciting twist, our research extends this theory to encompass $\\textit{any}$ norm. By delving into the relationship between the manifold's second fundamental form and the convex geometry of the norm's unit ball, we uncover the limiting differential operator for graph Laplacians built on various norms.\n\nWe showcase the advantages of non-Euclidean norms in manifold learning through a compelling case: mapping the dynamic movement of large, continuously variable molecules. Our numerical simulations reveal that a tweaked Laplacian eigenmaps algorithm, leveraging the Earthmover's distance, surpasses the traditional Euclidean approach. It not only slashes computational costs but also reduces the sample size required to accurately capture the intrinsic geometry.", "We introduce an efficient integral equation method to solve the heat equation \\(u_t(\\mathbf{x}) - \\Delta u(\\mathbf{x}) = F(\\mathbf{x}, t)\\) in a two-dimensional, multiply connected domain with Dirichlet boundary conditions. Instead of relying on heat kernel-based integral equations, we first discretize the problem in time. This results in a non-homogeneous modified Helmholtz equation that needs to be solved at each time step. The solution of this equation is expressed as a combination of a volume potential and a double layer potential. The volume potential is computed using a fast multipole-accelerated solver. To satisfy the boundary conditions, we solve an integral equation for the homogeneous modified Helmholtz equation, which is also accelerated by the fast multipole method (FMM). For a total of \\(N\\) points used in the discretization of the boundary and the domain, the overall computational cost per time step is \\(O(N)\\) or \\(O(N \\log N)\\).", "We explore a method involving sequential state-discrimination measurements on qudits to identify the quantum state they were originally in. These qudits are part of a set of non-orthogonal quantum states, making them indistinguishable with absolute certainty. Unambiguous state discrimination allows for error-free measurements but may sometimes fail to provide a definitive answer about the qudit's state. Qudits can potentially convey more information per transmission compared to qubits. We examined the scenario where Alice sends one of N qudits, where each qudit is of dimension N. We investigate two situations: one where all states have the same overlap, and another where qudits are divided into two groups with differing overlaps between sets. Additionally, we analyze the resilience of our method against a straightforward eavesdropping attempt and discovered that utilizing qudits instead of qubits increases the likelihood of error introduction by eavesdroppers, thereby making them more detectable.", "This study aims to enhance secure access control in the Hyperledger Fabric blockchain by integrating multiple IDs, attributes, and policies with the components that manage access control. Initially, the existing access control system in Hyperledger Fabric is thoroughly examined. Subsequently, a new implementation is proposed that enhances the current solution and offers users and developers simpler methods to make access control decisions using combinations of multiple IDs, attributes, and policies. Our proposed implementation includes encapsulating the Fabric CA client to facilitate adding attributes and streamlining the registration and enrollment of newly created certificates for new users. The research concludes that combining multiple IDs, attributes, and policies is feasible using Hyperledger Fabric's smart contract technology. Additionally, it was observed that the performance impact on real-world applications is minimal compared to the insecure scenario of always granting resource access without any access control.", "This work presents pyramidal convolution (PyConv), which can process input at multiple filter scales. PyConv contains a hierarchy of kernels, each with different filter sizes and depths, capturing various levels of detail in the scene. Despite its enhanced recognition capabilities, PyConv is efficient and does not increase computational cost and parameters compared to standard convolution. It is also highly flexible and extensible, offering a wide range of possible network architectures for various applications. PyConv has the potential to influence nearly every computer vision task. In this work, we introduce different PyConv-based architectures for four primary visual recognition tasks: image classification, video action classification/recognition, object detection, and semantic image segmentation/parsing. Our approach demonstrates significant improvements over these core tasks compared to baselines. For example, in image recognition, our 50-layer network outperforms a 152-layer ResNet baseline on the ImageNet dataset, with 2.39 times fewer parameters, 2.52 times lower computational complexity, and over 3 times fewer layers. In image segmentation, our novel framework sets a new state-of-the-art on the challenging ADE20K benchmark for scene parsing. Code is available at: https://github.com/iduta/pyconv", "The progress in the search for solar axions using the CERN Axion Solar Telescope (CAST) will be reviewed. Insights from the initial segment of CAST phase II, during which 4He gas at varying pressures filled the magnet bores to probe axion masses up to 0.4 eV, will be shared. In the absence of surplus X-rays when the magnet aligned with the Sun, we establish a typical upper limit on the axion-photon coupling of g < 2.17 x 10^-10 GeV^-1 at the 95% confidence level for axion masses under 0.4 eV, with exact results contingent on the pressure setting. Currently, phase II's second segment focuses on detecting axions with masses up to approximately 1.2 eV utilizing 3He as a buffer gas. Projections for sensitivity will be outlined. Additionally, near-term outlooks and longer-term strategies for a novel helioscope experiment will be discussed.", "Observations show that Arctic sea ice is rapidly retreating while Antarctic sea ice is steadily expanding. However, climate models usually predict a moderate decrease in sea ice for both regions. A small subset of models does reflect the observed trends, leading some studies to suggest that considering internal climate variability aligns models with observations. \n\nWe analyzed sea ice changes from 1979-2013 using data from CMIP5 and CESM-LE, based on the relationship between global-mean surface temperature and sea ice extent. Our findings indicate that simulations matching the observed Arctic sea ice retreat show significantly more global warming than observed. Using two methods to estimate sea ice retreat under observed global warming, we found such fast Arctic retreat in simulations occurs less than 1% of the time, suggesting models are inconsistent with observations.\n\nFor the Antarctic, simulations showing sea ice expansion align with insufficient global warming, with less certainty. This inconsistency means simulations fail to capture the observed asymmetry between Arctic and Antarctic sea ice trends, suggesting models are getting the right trends for the wrong reasons in both regions.", "Bio-features are rapidly becoming essential tools for authenticating IoT devices. This investigation aims to summarize the factors that hinder the development and deployment of biometric models on a large scale, including human physiological features (e.g., face, eyes, fingerprints, palm, or electrocardiogram) and behavioral features (e.g., signature, voice, gait, or keystroke). It also examines the various machine learning and data mining methods used by authentication and authorization schemes for mobile IoT devices. Additionally, we present the threat models and countermeasures employed by biometrics-based authentication schemes for these devices. Specifically, we analyze the current state of biometric-based authentication schemes for IoT devices. Based on the existing taxonomy, we conclude our paper by discussing the different challenges that future research efforts in biometrics-based authentication schemes for IoT devices must address.", "Device fingerprinting over the web has attracted significant attention from both researchers and the commercial market. Almost all existing fingerprinting techniques rely on software running on the device, which users can modify to evade detection. In this position paper, we argue that the advent of the HTML5 standard introduces a new class of fingerprinting features based on the device's hardware. These hardware-based features are much more challenging to alter or obscure, thereby providing a higher degree of confidence in the fingerprint. We propose several methods for HTML5 web applications to identify a device's hardware and present an initial experiment focused on fingerprinting a device's GPU.", "We present the partition function of Chern-Simons theory with the exceptional gauge group on a three-sphere, expressed as the partition function of the refined closed topological string. The relationship between the single K\u00e4hler parameter $\\tau$, string coupling constant $g_s$, and refinement parameter $b$ is given by $2\\tau=g_s(1-b)$, where $b$ takes specific values ($\\frac{5}{3}, \\frac{5}{2}, 3, 4, 6$) for the exceptional groups $G_2, F_4, E_6, E_7, E_8$ respectively. The key BPS invariants are $N^2_{0,\\frac{1}{2}}=1$ and $N^{11}_{0,1}=1$, and the partition function includes terms related to refined constant maps of string theory.\n\nThis derivation is based on the universal (in Vogel's sense) form of a Chern-Simons partition function on a three-sphere, restricted to the exceptional line $Exc$ where $\\gamma=2(\\\u03b1+\\\u03b2)$. This line encompasses all exceptional groups. Additionally, similar results apply to the $F$ line $\\gamma=\\\u03b1+\\\u03b2$ (which includes $SU(4), SO(10)$, and $E_6$ groups), with non-zero BPS invariants $N^2_{0,\\frac{1}{2}}=1$ and $N^{7}_{0,1}=1$.\n\nIn both cases, the refinement parameter $b$ (given by $-\\epsilon_2/\\epsilon_1$ in terms of Nekrasov's parameters) is expressed in terms of universal parameters, restricted to the relevant line, as $b=-\\\u03b2/\\\u03b1$.", "The centerpoint theorem is a prominent result in discrete geometry. It asserts that for any set of $n$ points in $\\mathbb{R}^d$, there exists a point $c$\u2014which need not be part of the original set\u2014such that every halfspace containing $c$ includes at least $\\frac{n}{d+1}$ points from the set. This point, called a centerpoint, can be considered a higher-dimensional generalization of a median, serving as a significant representative for the point set.\n\nHowever, what if we consider multiple representatives? In one-dimensional datasets, for instance, certain quantiles are often used as representatives instead of just the median. Here, we explore extending the concept of quantiles to higher dimensions. Specifically, we aim to identify a set $Q$ of a few points such that any halfspace containing at least one point from $Q$ also contains a substantial fraction of the original point set, $P$. Additionally, any halfspace encompassing more points from $Q$ will contain an even larger fraction of $P$.\n\nThis approach aligns with established concepts like weak $\\varepsilon$-nets and weak $\\varepsilon$-approximations. It is more comprehensive than the former while not as stringent as the latter.", "Introducing our groundbreaking software package, \\textsc{PsrPopPy}, crafted for simulating pulsar populations. This innovation is an evolution of the \\textsc{Psrpop} package, meticulously re-engineered in Python while preserving essential Fortran libraries for specific tasks. Leveraging Python's object-oriented capabilities, we've enhanced the modularity and flexibility of the codebase significantly. \n\nFor ease of use, we\u2019ve included pre-written scripts enabling 'standard' operational simulations, yet the design accommodates the crafting of custom scripts, enabling personalized exploration. The modular architecture simplifies the integration of experimental features, such as new period or luminosity distribution models, making it more intuitive than ever. \n\nWe delve into potential future enhancements of the software, illustrating its versatility and power with practical examples. Utilizing survey data from various observing frequencies, we establish that pulsar spectral indices conform to a normal distribution with a mean of -1.4 and a standard deviation of 1.0. Further, we model pulsar spin evolution to refine the relationship between a pulsar's luminosity and its spin parameters.\n\nBy replicating and optimizing the analysis of Faucher-Gigu\u00e8re & Kaspi, we've honed their power-law dependence between radio luminosity ($L$), period ($P$), and period derivative ($\\dot{P}$). Our findings reveal an underlying population best described by $L \\propto P^{-1.39 \\pm 0.09} \\dot{P}^{0.48 \\pm 0.04}$, closely aligning with results for $\\gamma$-ray pulsars by Perera et al.\n\nArmed with this refined relationship, we generate a model population, exploring the age-luminosity dynamics for pulsars. These insights are expected to be measurably significant following future large-scale surveys with the Square Kilometer Array, heralding a new era in pulsar population studies.", "We investigate the dynamics of a spin ensemble strongly coupled to a single-mode resonator driven by external pulses. When the mean frequency of the spin ensemble resonates with the cavity mode, we observe damped Rabi oscillations between the spin ensemble and the cavity mode. Our analysis, which accurately accounts for the dephasing effect of inhomogeneous spin broadening, reveals that precise knowledge of this broadening is essential for both qualitative and quantitative understanding of the spin-cavity temporal dynamics. Based on this insight, we demonstrate that coherent oscillations between the spin ensemble and the cavity can be significantly enhanced\u2014by several orders of magnitude\u2014when the system is driven with pulses matching specific resonance conditions. Our theoretical approach is validated by an experiment involving an ensemble of negatively charged nitrogen-vacancy (NV) centers in diamond, strongly coupled to a superconducting coplanar single-mode waveguide resonator.", "We study the ground-state Riemannian metric and cyclic quantum distance of an inhomogeneous quantum Ising spin-1/2 chain in a transverse field. By applying a general canonical transformation, we diagonalize the model's fermionic Hamiltonian mapped from the spin system. The exact ground-state Riemannian metric on a parameter manifold ring $S^1$ is obtained using a gauge transformation with a twist operator. We explore the ground-state cyclic quantum distance and the second derivative of the ground-state energy across different inhomogeneous exchange coupling regions. Notably, we demonstrate that the quantum ferromagnetic phase in the uniform Ising chain is marked by an invariant cyclic quantum distance and a constant ground-state Riemannian metric, which rapidly diminishes to zero in the paramagnetic phase.", "Rotation measure synthesis facilitates the estimation of Faraday dispersion through a Fourier transform, serving as a central method for investigating cosmic magnetic fields. We demonstrate that this process is mathematically comparable to the one-dimensional interferometric intensity measurement equation, albeit within a different Fourier space. Consequently, established concepts from two-dimensional intensity interferometry, which are designed to address various instrumental conditions, can be applied to Faraday dispersion analysis. Specifically, we illustrate a model for the impact of channel averaging during Faraday reconstruction, a factor that has previously hindered advancements in polarimetric science using wide-band measurements. Additionally, we simulate one-dimensional sparse reconstruction with channel averaging for realistic frequency ranges, showing the feasibility of detecting signals with high rotation measure values that were previously undetectable. This is particularly crucial for low-frequency and wide-band polarimetry. We extend these concepts to include mosaicking in Faraday depth into the channel averaging process. This work provides the first comprehensive framework for accurately performing wide-band rotation measure synthesis, including the integration of data from multiple telescopes, which is expected to significantly enhance the quality and scope of polarimetric research. This advancement is particularly relevant for extreme environments with high magnetic fields, such as those associated with pulsars and Fast Radio Bursts (FRBs), and will enable these sources to be effectively used as probes of cosmological magnetic fields.", "The study investigates the characteristic properties of charged particle production in hadron-nucleus collisions at high energies using various statistical models. Predictions derived from different approaches, including the Negative Binomial distribution, the shifted Gompertz distribution, the Weibull distribution, and the Krasznovszky-Wagner distribution, are employed for a comparative analysis of these models' relative successes. These distributions, originating from diverse functional forms, are based on either phenomenological parameterizations or theoretical models of the underlying dynamics. Several of these models have also been utilized to analyze data from the LHC for both proton-proton and nucleus-nucleus collisions. The analysis employs a range of physical and derived observables.", "In 1975, John Tukey introduced a groundbreaking concept known as the multivariate median\u2014the 'deepest' point within a data cloud in R^d. This sparked a revolution in how we understand data. Building on Tukey's idea, David Donoho and Miriam Gasko later explored how to measure the depth of any point z in relation to a dataset. By examining hyperplanes passing through z, they determined its 'depth' based on the smallest fraction of the data separated by such a hyperplane. \n\nSince then, this pioneering concept has blossomed into a rich statistical methodology centered on data depth and more broadly, nonparametric depth statistics. Various notions of data depth, both general and specific, have emerged. These concepts differ in terms of their computability, robustness, and sensitivity to capturing asymmetrical shapes within the data, making them suitable for different applications.\n\nThe upper-level sets of a depth statistic generate a family of set-valued statistics termed depth-trimmed or central regions. These regions offer a detailed description of a distribution's location, scale, and shape, with the most central region serving as a median. The idea of depth has not only been extended from empirical data clouds to general probability distributions in R^d, supporting laws of large numbers and consistency results, but also to data in functional spaces.\n\nThis evolving field continues to provide valuable insights and tools for analyzing complex datasets, pushing the boundaries of statistical science.", "Strain-engineering in SiGe nanostructures is crucial for the development of nanoscale optoelectronic devices. This study introduces a novel approach wherein SiGe structures are laterally confined by a Si substrate to achieve high tensile strain without external stressors, thereby enhancing scalability. We employ spectro-microscopy techniques, finite element method simulations, and ab initio calculations to investigate the strain state of laterally confined Ge-rich SiGe nano-stripes. Using tip-enhanced Raman spectroscopy with an exceptional lateral resolution of ~30 nm, we obtain detailed strain information. The nano-stripes display a significant tensile hydrostatic strain component, peaking at the center of the top free surface and diminishing at the edges. The maximum lattice deformation exceeds typical values found in thermally relaxed Ge/Si(001) layers. This strain enhancement arises from frustrated relaxation in the out-of-plane direction, driven by a combination of lateral confinement from the substrate sidewalls and plastic relaxation of misfit strain in the (001) plane at the SiGe/Si interface. The tensile lattice deformation at the stripe surface is examined through work function mapping, utilizing X-ray photoelectron emission microscopy with spatial resolution better than 100 nm. Results indicate a positive work function shift relative to bulk SiGe alloy, confirmed through electronic structure calculations of tensile strained configurations. These findings have significant implications for the design of optoelectronic devices at the nanometer scale.", "During an infectious disease pandemic, it is imperative to disseminate electronic medical records or derived models across regions. However, employing data or models from one region in another often presents distribution shift challenges that contravene the assumptions inherent in traditional machine learning methodologies. Transfer learning offers a promising solution to these challenges. To investigate the efficacy of deep transfer learning algorithms, we employed two data-centric approaches\u2014domain adversarial neural networks and maximum classifier discrepancy\u2014and model-centric transfer learning algorithms in the context of infectious disease detection.\n\nAdditionally, we examined well-defined synthetic scenarios where inter-regional data distribution discrepancies were explicitly known. Our experimental findings indicate that, within the realm of infectious disease classification, transfer learning is advantageous under specific conditions: (1) when the source and target datasets are analogous, yet the target training dataset is limited, and (2) when the target training dataset lacks labeling. In the former scenario, model-based transfer learning demonstrated comparable performance to data-based transfer learning models.\n\nNonetheless, further examination of domain shift in real-world research data is essential to address the observed decline in model performance.", "Bound states in the continuum (BIC) have been a focal point of optics and photonics research over the past decade. It is particularly intriguing to explore the effects associated with quasi-BICs in the simplest structures, where quasi-BICs are most pronounced, such as dielectric cylinders. Previous studies have investigated quasi-BICs both in single cylinders and in arrays composed of multiple cylinders. \n\nIn this work, we examine the properties of quasi-BICs as we transition from a homogeneous dielectric cylinder in an air environment to a ring with narrow walls, gradually increasing the diameter of the inner air cylinder. The results reveal a transition in quasi-BICs from a strong-coupling to a weak-coupling regime. This transition is characterized by a shift from avoided crossing of branches to their intersection, with the quasi-BIC preserved only on one straight branch.\n\nIn the strong-coupling regime, three waves interfere in the far-field zone: two waves corresponding to the resonant modes of the structure and a wave scattered by the structure as a whole. We also discuss the applicability of the Fano resonance concept, which describes the interference of only two waves under weak coupling conditions.", "Turbulent thermal diffusion arises from the interplay between temperature-stratified turbulence and the inertia of small particles. This phenomenon leads to a non-diffusive turbulent flux of particles moving in the direction of the turbulent heat flux. This flux is proportional to the product of the mean particle number density and the effective velocity of inertial particles. Previous theories addressing this effect have been limited to conditions with small temperature gradients and Stokes numbers (Phys. Rev. Lett. {\\bf 76}, 224, 1996). In this research, a comprehensive theory covering a wider range of temperature gradients and Stokes numbers has been developed. Experiments conducted with oscillating grid turbulence and multi-fan generated turbulence were used to test the theory under strongly stratified turbulent conditions. The findings indicate that for high Reynolds numbers, the effective velocity of inertial particles is less than the characteristic vertical turbulent velocity. Both the effective velocity of inertial particles and the effective coefficient of turbulent thermal diffusion increase with rising Stokes numbers, peaking at low Stokes numbers and diminishing for higher values. Additionally, the effective coefficient of turbulent thermal diffusion decreases as the mean temperature gradient increases. The newly developed theory aligns well with the experimental results.", "The visibility of pulsar radio emission is traditionally modeled based on the assumption that the emission is confined within a narrow cone aligned with the tangent to a dipolar magnetic field line. A widely accepted approximation, known as the Rotating Vector Model (RVM), simplifies the scenario to a fixed line of sight where the field line is not exactly tangential to the observational path. However, a more precise approach, referred to as the tangent model (Gangadhara 2004), considers that the visible emission point varies with the pulsar rotational phase, denoted by $\\psi$, describing a specific trajectory on a sphere with radius $r$. We have solved for this trajectory and calculated the angular velocity of the visible point along it.\n\nRecent advancements suggest that this motion can be observed using interstellar holography, as noted by Pen et al. (2014). On evaluating the RVM's accuracy, we find that it introduces significant errors, particularly for pulsars with wide-ranging emission over $\\psi$. The RVM tends to underestimate the $\\psi$ range within which emission is detectable. Additionally, geometric considerations indicate that visible pulsar radio emissions likely originate from heights exceeding ten percent of the light-cylinder distance. At such heights, the effects of retardation become non-negligible, which our current model does not account for.", "In image recognition, training samples often fail to cover all possible target classes. Zero-shot learning (ZSL) employs class semantic information to classify samples from unseen categories not represented in the training set. This paper introduces the Global Semantic Consistency Network (GSC-Net), an end-to-end framework that fully utilizes the semantic information from both seen and unseen classes for efficient zero-shot learning. Additionally, we implement a soft label embedding loss to better leverage the semantic relationships among classes. To make GSC-Net suitable for the more practical Generalized Zero-shot Learning (GZSL) scenario, we incorporate a parametric novelty detection mechanism. Our method achieves state-of-the-art performance in both ZSL and GZSL tasks across three visual attribute datasets, demonstrating the effectiveness and advantages of the proposed framework.", "The common belief that Category Theory underpins Mathematical Structuralism is mistaken. Instead, the foundations of mathematics through Category Theory necessitate a distinct philosophical approach. Unlike structural mathematics, which focuses on invariant forms (as per Awodey), categorical mathematics explores covariant transformations that typically lack invariants. In this paper, I propose a non-structuralist perspective on categorical mathematics and discuss its implications for the history of mathematics and mathematics education.", "In this study, we demonstrate that within a non-equilibrium exciton-polariton condensate system, where polaritons are produced through incoherent pumping, a ring-shaped pump facilitates the creation of stationary vortex memory elements characterized by a topological charge of \\( m = 1 \\) or \\( m = -1 \\). Furthermore, by employing straightforward potential guides, we can selectively replicate the same charge or invert it onto another spatially distinct ring pump. This capability for manipulating binary information suggests the potential for a novel type of processing that leverages vortices as topologically protected memory components.", "During the three-year assessment phase of the LOFT mission, a candidate for the ESA Cosmic Vision program's M3 launch opportunity, we evaluated the radiation damage to the silicon drift detectors (SDDs) in the satellite's instrumentation. We irradiated the detectors with 0.8 and 11 MeV protons to analyze the increase in leakage current and changes in charge collection efficiency due to displacement damage. We also exposed the detectors to hypervelocity dust grains to assess the impact of debris. This paper describes our measurements and discusses the results within the context of the LOFT mission.", "In this paper, we take a closer look at how low-level multimodal features can be used to find movie similarities for content-based movie recommendations. We specifically show how to create multimodal representations of movies using subtitles, audio cues, and visual elements. For textual data, we focus on topic modeling from subtitles to identify themes that distinguish movies. On the visual side, we extract meaningful features that capture camera movements, colors, and faces. For audio, we use basic classification from pretrained models.\n\nWe combine these three elements\u2014text, audio, and visual\u2014with static metadata like directors and actors to enhance the process of finding similar movies. To demonstrate our approach, we created a small dataset of 160 well-known movies, and we used this data to generate recommendation rankings based on the different types of information we combined.\n\nOur extensive experiments show that integrating text, audio, and visual data significantly boosts the performance of content-based recommendation systems by more than 50% compared to using just metadata. As far as we know, this is the first time a method has combined such a wide range of features from all these different types of data to improve content similarity estimation over traditional metadata approaches.", "We investigate the radiation emitted by a Reissner-Nordstrom black hole with electric charge within the context of quantum gravity. Utilizing canonical quantization for spherically symmetric geometries and adhering to physically reasonable assumptions, we solve the Wheeler-De Witt equation not only in the region extending from the outer apparent horizon to spatial infinity but also from the spacetime singularity to the inner apparent horizon. Our findings reveal that the mass loss rate of a black hole evaporating due to thermal radiation matches the semiclassical predictions when an integration constant is chosen appropriately based on physical principles. Additionally, we solve the Wheeler-De Witt equation in the region between the inner Cauchy horizon and the outer apparent horizon, demonstrating that the expression for the mass loss rate of the evaporating black hole remains consistent. This study represents a natural extension from the uncharged Schwarzschild black hole scenario to the charged Reissner-Nordstrom black hole case.", "We present multi-agent A* (MAA*), the first complete and optimal heuristic search algorithm for decentralized partially-observable Markov decision problems (DEC-POMDPs) with a finite horizon. This algorithm computes optimal plans for cooperative agents in stochastic environments, such as multirobot coordination, network traffic control, and distributed resource allocation. Addressing these problems is a major challenge in planning under uncertainty. Our solution combines classical heuristic search and decentralized control theory. Experimental results highlight MAA*'s significant advantages. We also introduce an anytime variant and discuss potential extensions for solving infinite horizon problems.", "We demonstrate morphological classifications generated through machine learning for objects in SDSS DR6, previously classified by Galaxy Zoo into early types, spirals, and point sources/artifacts. An artificial neural network (ANN) is trained using a subset of human-classified objects, and we assess whether the algorithm can replicate the human classifications for the remaining sample. The success of the ANN in matching human classifications is highly dependent on the chosen input parameters for the algorithm. While colors and profile-fitting parameters are effective in differentiating the objects into three classes, incorporating adaptive shape parameters, concentration, and texture significantly improves the results. Alone, adaptive moments, concentration, and texture parameters cannot distinguish early-type galaxies from point sources/artifacts. Utilizing a set of twelve parameters, the ANN achieves over 90% accuracy in replicating human classifications for all three morphological classes. Even an incomplete magnitude training set does not undermine our findings, given our specific choice of network input parameters. We conclude that machine-learning algorithms show promise for morphological classification in future extensive imaging surveys, with the Galaxy Zoo catalog serving as a valuable training resource.", "The Lambek calculus is a well-known logical formalism for modeling natural language syntax. The original calculus addressed a substantial number of intricate natural language phenomena, but only within the context-free setting. To tackle more subtle linguistic issues, the Lambek calculus has been extended in various ways. Notably, Morrill and Valentin (2015) introduced an extension that includes exponential and bracket modalities. This extension is based on a non-standard contraction rule for the exponential, which interacts intricately with the bracket structure. The standard contraction rule is not admissible in this calculus. In this paper, we prove the undecidability of the derivability problem in their calculus. We also examine restricted decidable fragments considered by Morrill and Valentin and demonstrate that these fragments fall within the NP class.", "The transition between the two phases of 4D Euclidean Dynamical Triangulation was long believed to be of second order. However, in 1996, a first-order behavior was discovered in sufficiently large systems. This finding raised questions about the influence of the numerical methods used: to control volume fluctuations, both studies added an artificial harmonic potential to the action, and in one study, measurements were taken after a fixed number of accepted moves instead of attempted moves, introducing additional error. Moreover, the simulations were affected by strong critical slowing down, which might have been underestimated.\n\nIn the present work, we address these weaknesses by allowing the volume to fluctuate freely within a fixed interval, taking measurements after a fixed number of attempted moves, and using an optimized parallel tempering algorithm to overcome critical slowing down. With these improved methods, on systems of up to 64k 4-simplices, we confirm that the phase transition is first order. Additionally, we discuss a local criterion for deciding whether parts of a triangulation are in the elongated or crumpled state and describe a new correspondence between EDT and the balls-in-boxes model. This model leads to a modified partition function with an additional, third coupling. Finally, we propose a class of modified path-integral measures that might remove the metastability of the Markov chain and convert the phase transition into second order.", "Sure, here's a friendlier version:\n\nWe describe the virtually nilpotent finitely generated groups (or, as Gromov's theorem states, groups with polynomial growth) for which the Domino Problem can be solved. These include the virtually free groups, meaning finite groups, and those that have $\\Z$ as a subgroup with a finite index.", "Gamma rays from dark matter particle annihilation in the Galactic halo present a highly promising method for indirect dark matter detection. We show that distinct spectral features at energies close to the dark matter particle\u2019s mass, predicted by most models, can greatly enhance gamma-ray telescope sensitivity to dark matter. Our projected limits on these features, including the commonly sought line signals, demonstrate their superior efficiency in constraining dark matter's nature compared to the broader, model-independent spectral features at lower energies.", "Achieving carbon neutrality necessitates a research agenda that tackles the technical and economic challenges faced as we move toward complete reliance on renewable electricity generation. Increasing shares of variable renewable energy (VRE) sources (such as wind turbines and photovoltaic systems) complicate the balance between supply and demand in VRE-dominated power grids. The operational features and impacts of VRE inverters also need attention. In this context, we explore the consequences of the transition to carbon neutrality and outline the related research challenges in system planning, operation, and stability. We emphasize the importance of energy storage integration, demand-side participation, distributed control and estimation, and the interconnection of various energy sectors. Additionally, we identify gaps in the current literature and discuss our recent research aimed at bridging these gaps, thereby improving grid operation and assessment. We provide numerical outcomes from comparative case studies on the operational stability and economics of power grids with significant VRE sources, aiding stakeholders in crafting specific roadmaps and making informed decisions.", "Convolutional neural networks (CNNs) are increasingly used in various areas of computer vision due to their ability to process large amounts of labeled data through millions of parameters. However, as CNN models grow larger, their storage and memory requirements also increase. We introduce a novel network architecture called Frequency-Sensitive Hashed Nets (FreshNets), which significantly reduces memory and storage consumption by exploiting redundancy in the convolutional and fully-connected layers of deep learning models. \n\nOur approach is based on the observation that the weights of learned convolutional filters are usually smooth and low-frequency. We first convert these filter weights to the frequency domain using a discrete cosine transform (DCT). Then, we apply a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned to the same hash bucket share a single value learned through standard back-propagation. To further reduce model size, we allocate fewer hash buckets to high-frequency components, which are generally less important.\n\nWe evaluated FreshNets on eight datasets and found that it achieves significantly better compression performance compared to several relevant baselines.", "We carried out a Project-Based Learning (PBL) activity that integrated Japanese cartoon (manga) techniques into the Requirements Development (RD) process. Manga possesses well-defined methods for character development and storytelling, which we believed could be effectively applied to RD. Through this manga-driven approach, students managed to clearly identify overarching project objectives early in the development cycle and successfully devised high-quality, innovative system concepts.", "When a black hole evaporates, it eventually disappears. To understand this better, we're looking at how particles called fermions escape from a 5-dimensional rotating black string, which is like a stretched-out black hole. The temperature of the black string is influenced not just by itself, but also by the properties of the escaping fermions and the extra spatial dimension they travel through. Because of quantum effects, the temperature increase slows down, which means that the black string doesn't fully evaporate and leaves behind a remnant.", "In our study, we introduce second-order vector representations of words derived from the topological features of nearest neighbors in pre-trained contextual word embeddings. We then explore the impact of using these second-order embeddings as input features in three models: two deep natural language processing models for named entity recognition and recognizing textual entailment, and a linear model for paraphrase recognition.\n\nOur surprising findings reveal that nearest neighbor information alone can capture most of the performance advantages typically associated with pre-trained word embeddings. Second-order embeddings, while less specific, excel in handling highly heterogeneous data compared to first-order representations. Moreover, integrating second-order information into contextual embeddings can enhance model performance in certain scenarios.\n\nWe also observe that leveraging nearest neighbor features from multiple first-order embedding samples, given the variance in random initializations, can further boost downstream performance. Finally, we highlight fascinating attributes of second-order embedding spaces, such as higher density and unique semantic interpretations of cosine similarity, warranting further investigation.", "Reconfigurable intelligent surface (RIS)-aided millimeter wave wireless systems exhibit enhanced robustness to blockage and extended coverage. In this study, we investigate the potential of RIS to additionally furnish improved localization capabilities as a collateral benefit of communication. We employ sparse reconstruction algorithms to derive high-resolution channel estimates, which are subsequently translated into position information. In RIS-aided mmWave systems, the complexity inherent in sparse recovery becomes a limiting factor, due to the extensive number of RIS elements and expansive communication arrays. We propose leveraging a multidimensional orthogonal matching pursuit strategy for compressive channel estimation within a RIS-aided millimeter wave framework. This algorithm, which computes projections on a series of independent dictionaries rather than a singular, extensive dictionary, facilitates high-precision channel estimation with reduced complexity. Furthermore, we integrate this strategy with a localization method that does not depend on the absolute time of arrival of the Line-of-Sight (LoS) path. Localization analyses conducted in a realistic three-dimensional indoor environment demonstrate that RIS-aided wireless systems can also achieve marked enhancements in localization accuracy.", "Ensuring confidentiality requires detecting and quantifying information leaks through timing side channels. While static analysis is the go-to method for spotting these side channels, it's tough to apply in real-world scenarios. Plus, detection techniques often just give a binary 'yes' or 'no' answer. But real-world situations might actually need to leak information. That's where quantification comes in handy to assess the risks posed by such leaks. Given the limitations of static analysis, we've come up with a dynamic analysis method. Our innovative approach breaks the problem down into two tasks: first, we train a neural network to model the program's timing, and then we analyze this network to quantify the leaks. Our experiments show both tasks are practically viable, offering a major leap over current side channel detection and quantification tools. Our main technical achievements include developing a neural network architecture that detects side channels and creating an MILP-based algorithm to measure side-channel strength. We tested our approach on micro-benchmarks and real-world apps, proving that neural networks can learn the timing behaviors of programs with thousands of methods and efficiently identify and quantify leaks through timing side channels.", "The inner asteroid belt, spanning 2.1 to 2.5 astronomical units (au), holds particular dynamical significance as it predominantly sources both chondritic meteorites and near-Earth asteroids. This region is delineated by an eccentricity-type secular resonance and the 1:3 mean motion resonance with Jupiter. Unless asteroids possess perihelia sufficiently low to allow scattering by Mars, their escape necessitates transport to one of these bounding resonances. Additionally, Yarkovsky forces are generally ineffective in altering the eccentricity and/or inclination of asteroids with diameters greater than or equal to 30 km. Consequently, large asteroids with perihelia significantly distant from Mars may only exit the inner belt due to substantial changes in their eccentricities.\n\nIn this paper, we systematically examine the chaotic diffusion of orbits near the 1:2 mean motion resonance with Mars. We demonstrate that although chaotic orbital evolution\u2014affecting both resonant and non-resonant orbits\u2014induces increased dispersion of inclinations and eccentricities, it does not substantially alter their mean values. Moreover, we find that while dispersive growth is most pronounced for resonant orbits, at high eccentricities the resonance acts to reduce asteroid scattering by Mars, thereby extending the asteroid's lifespan within the belt compared to non-resonant orbits. For asteroids of all sizes in both resonant and non-resonant orbits, the eccentricity changes required to match observations cannot be attributed to gravitational forces alone. We also analyze the role of resonant trapping in safeguarding asteroids from Mars encounters.", "The impact of nonstandard neutrino interactions (NSI) is profound, shaking up the precision measurements at the next wave of neutrino oscillation experiments! To get a clearer picture and pin down the elusive NSI parameter space, we need a variety of experimental setups. Our focus zooms in on the constraints on NSI with electrons at both current and upcoming $e^+e^-$ collider experiments such as Belle II, STCF, and CEPC.\n\nThe results are electrifying! Belle II and STCF are set to deliver competitive and complementary bounds on electron-type NSI parameters, giving the current global analysis a run for its money. Moreover, they promise significant leaps forward in constraining tau-type NSI. But wait, there's more: CEPC could single-handedly clamp down on the NSI parameter space with electrons with impressive precision.\n\nIn an exciting twist, by blending data from three distinct running modes, we can untangle the left-handed (vector) and right-handed (axial-vector) NSI parameters. The allowed ranges for $|\\epsilon_{ee}^{eL}|$ ($|\\epsilon_{ee}^{eV}|$) and $|\\epsilon_{ee}^{eR}|$ ($|\\epsilon_{ee}^{eA}|$) can be squeezed to less than 0.002 at CEPC, even if both parameters coexist. The future of neutrino research looks brighter than ever!", "The Deep Underground Neutrino Experiment (DUNE) is a cutting-edge initiative aimed at advancing neutrino science and probing proton decay. Its far detector will feature four 10-kiloton Liquid Argon (LAr) Time Projection Chambers, utilizing both single and dual-phase technologies, with the latter providing charge amplification in the gaseous phase. To refine these designs, two sizable prototypes have been gathering data at CERN since 2018. Earlier, a dual-phase 4-tonne demonstrator was constructed and tested with cosmic muons in 2017, demonstrating excellent charge and light collection performance. The light detection system plays a crucial role in triggering the charge acquisition system and gleaning additional information from the scintillation light produced during particle interactions. In the demonstrator, five cryogenic photo-multipliers were tested with various base polarity configurations and wavelength shifting techniques. Throughout the detector's operation, scintillation light data were collected under different drift and amplification field conditions. This overview highlights the light detection system's performance and provides insights into light production and propagation, ultimately enhancing our understanding of LAr properties.", "Major chip manufacturers have introduced multithreaded processors designed for diverse workloads, emphasizing efficient resource utilization. Key to their performance is leveraging memory-level parallelism (MLP). This paper proposes an MLP-aware operating system (OS) scheduling algorithm for multithreaded multi-core processors. By monitoring the MLP of each thread and balancing it with system resources, the OS can generate optimized thread schedules for improved performance in the next quantum. We qualitatively compare our solution with existing hardware and software techniques, suggesting that future work should focus on quantitative evaluations and further optimizing the scheduling algorithm.", "We address the challenge of calibrating a compressed sensing measurement system where decalibration involves unknown gains for each measurement. Our focus is on blind calibration, which uses measurements from a few unknown, sparse signals. Although a naive approach to this blind calibration problem via $\\ell_{1}$ minimization shares similarities with blind source separation and dictionary learning\u2014both known for their non-convexity and local minima issues\u2014we demonstrate that in this specific context, the problem can be redefined as a convex optimization problem. This allows it to be solved with standard algorithms. Numerical simulations confirm the efficacy of our approach, even with highly uncalibrated measurements, provided that a sufficient number of sparse calibrating signals are available. We also observed that the method\u2019s success exhibits sharp phase transitions.", "We delve into the intriguing challenge of multi-source morphological reinflection, an extension of the traditional single-source version. Here, the input comprises two components: (i) a target tag and (ii) several pairs of source forms and source tags associated with a lemma. The driving force behind this approach is the advantage of accessing multiple source forms, as they can provide complementary insights, such as different stems. To tackle this problem more effectively, we introduce an innovative enhancement to the encoder-decoder recurrent neural network architecture, which incorporates multiple encoders. Our findings demonstrate that this advanced architecture surpasses single-source reinflection models. Additionally, we release our dataset for multi-source morphological reinflection to support and inspire future research in this area.", "As the demand for timely extraction of complex knowledge from semantically annotated data streams rises, particularly on the Web and in the Internet of Things (IoT), conventional methods face the challenge of performing expressive reasoning on these vast streams efficiently. Enter Laser, our innovative reasoning system that leverages a pragmatic and intricate fragment of the LARS logic\u2014an extension of Answer Set Programming (ASP) tailored for streams. Laser is built around a groundbreaking evaluation procedure that cleverly annotates formulae, eliminating the need to recompute duplicates at various time points. This approach, coupled with an astute implementation of LARS operators, results in markedly enhanced runtimes compared to leading systems like C-SPARQL, CQELS, or even a LARS version running on the ASP solver Clingo. Consequently, Laser enables the application of sophisticated logic-based reasoning to extensive data streams, unlocking a wider array of stream reasoning use cases.", "The second law of thermodynamics establishes the fundamental constraints on the exchange of energy and information between physical systems. In this study, we broaden a thermodynamic framework that describes this energy and information flow, previously developed for bipartite systems, to encompass multiple multipartite systems. We define a natural thermodynamic quantity that characterizes the information exchanged among these systems and subsequently present a refined version of this quantity. Our findings are demonstrated through a model involving two competing Maxwell demons.", "The intrinsic spin-orbit coupling (SOC) in graphene is typically negligible, but it can be enhanced by proximity effects in stacked heterostructures of graphene and transition metal dichalcogenides (TMDCs). The composition of the TMDC layer significantly influences the nature and strength of the induced SOC in the graphene layer. In this study, we investigate how the proximity-induced SOC evolves when defects are deliberately introduced into the TMDC layer. We simulate alloyed ${\\rm G/W_{\\chi}Mo_{1-\\chi}Se_2}$ heterostructures with varying compositions ($\\chi$) and defect distributions using density functional theory. By comparing these results with continuum and tight-binding models, we identify both local and global effects of metal-atom alloying. Our findings indicate that, despite significant local perturbations due to individual defects, the overall low-energy spin and electronic behavior can be described by a simple effective medium model, which depends solely on the composition ratio of the metallic species in the TMDC layer. Additionally, we demonstrate that the topological state of these alloyed systems can be tuned by adjusting this composition ratio.", "Atomic masses are essential for numerous nuclear astrophysics calculations. The absence of experimental values for pertinent exotic nuclides has spurred the swift advancement of new mass measurement devices globally. Time-of-Flight (TOF) mass measurements serve as a complementary method to the highly precise Penning trap measurements, which are restricted by the rate and half-lives of the ions under study. The NSCL facility offers an optimal setting for TOF mass measurements of extremely exotic nuclei. Recently at this facility, we introduced a TOF-Brho technique and conducted mass measurements of neutron-rich nuclides in the Fe region, which are critical for r-process calculations and for understanding processes in the crust of accreting neutron stars.", "Even though super-massive black holes (AGNs) and stellar mass black holes (XRBs) share many features, the broad emission lines (BELs) are a unique signature of AGNs. According to data from the SDSS, it appears there aren't any AGNs with a mass less than about 10^5 times the mass of our sun. \n\nIn this paper, we explore whether these low-mass black holes truly don't exist or if they simply go undetected because they don't produce BELs efficiently. By examining the ionizing spectral energy distribution across a range of black hole masses\u2014from 10 to 10^9 times the mass of our sun, covering XRBs to AGNs\u2014we calculated the equivalent widths (EWs) of several ultraviolet and optical lines: Ly\u03b1 1216 \u00c5, H\u03b2 4861 \u00c5, CIV 1549 \u00c5, and MgII 2798 \u00c5. \n\nWe used the LOC (locally optimally emitting cloud) model to describe the broad emission line region (BELR) for these calculations. Our findings indicate that the hardening of the spectral energy distribution with decreasing black hole mass does not reduce the BEL EWs. Instead, the production of these emission lines is regulated by the finite size of the BELR, dictated by the black hole's mass. \n\nWe noticed a peak in the EWs of emission lines for typical AGN black holes around 10^8 times the mass of the sun. Below this mass, the lines become significantly fainter, with a sharp drop-off occurring below around 10^6 times the mass of the sun. This sharp decline might explain why low-mass AGNs are missing in the SDSS data.", "For the first time ever, we've tested the precision of synchronization algorithms inspired by pulse-coupled oscillators on FPGA-based radios. And guess what? The results are impressive! These algorithms can achieve precision in the low microsecond range when implemented right in the physical layer. But that's not all\u2014we've taken it a step further by introducing an extension to the algorithm that accounts for phase rate deviations in the hardware. With this upgrade, we\u2019re seeing precision improve to below one microsecond!\n\nWhat does this mean in the real world? Our super-precise algorithm is perfect for ad hoc wireless systems, where it can handle fully distributed synchronization of transmission slots or sleep cycles. This is especially useful when there's no way to have centralized synchronization. It's a game-changer for making sure everything stays in sync, even in the most challenging and dynamic wireless environments.", "Human Trajectory Prediction (HTP) has garnered significant attention in recent years, leading to the development of numerous solutions. Proper benchmarking is crucial for comparing different methods. This paper tackles the challenge of evaluating the complexity of datasets in relation to the prediction problem. To assess dataset complexity, we define a series of indicators based on three key concepts: Trajectory Predictability, Trajectory Regularity, and Context Complexity. We analyze the most commonly used HTP datasets using these indicators and discuss the implications for benchmarking HTP algorithms. Our source code is available on GitHub.", "This paper aims to demonstrate how classical linear stochastic systems can be built using quantum optical components. Quantum optical systems offer higher bandwidth than electronic devices, resulting in faster response and processing times, and potentially better performance. The paper outlines a method for constructing these quantum optical systems and discusses their use in measurement feedback loops. Examples are provided to illustrate the main findings.", "Systems biology leverages extensive networks of biochemical reactions to simulate the operations of biological cells, ranging from the molecular level to the entire cell. The behavior of dissipative reaction networks with numerous distinct timescales can be depicted as a series of successive equilibrations of various subsets of the system's variables. For polynomial systems with distinct timescales, equilibrations occur when at least two opposing monomials are of the same order of magnitude and overshadow the others. These equilibrations, along with the resulting simplified dynamics achieved by disregarding the less significant terms, are naturally expressed through tropical analysis and are useful for model simplification.", "We analyzed the Suzaku data from the galactic disk and outflow areas of the starburst galaxy M82. For the central disk regions, our thermal modeling showed the need for at least three different temperature components. Interestingly, the Ly$\\beta$ line fluxes for O VIII and Ne X were higher than what we'd expect from a plasma in collisional ionization equilibrium. The Ly$\\beta$/Ly$\\alpha$ line ratios for these elements were also elevated, hinting at the presence of charge exchange.\n\nIn the outflow wind region, we found that the spectra fit well with two-temperature thermal models, and we were able to determine the metal abundances of O, Ne, Mg, and Fe there. The ratios of O/Fe, Ne/Fe, and Mg/Fe were approximately 2, 3, and 2 times the solar values reported by Lodders (2003). Since there's no sign of charge exchange in the outflow region, these metal abundances are likely more accurate compared to those in the central region. This pattern suggests that the starburst activity in M82 is enriching the outflow by ejecting metals from SN II into intergalactic space.", "Dust grains are usually believed to form in the winds of asymptotic giant branch (AGB) stars. However, more evidence now suggests that dust also forms in supernovae (SNe). To understand which source is more important for dust formation, we need to know how much newly-formed dust in SN ejecta can survive the reverse shock and enter the interstellar medium.\n\nWe have developed a new code called GRASH_Rev that tracks the dust evolution during a supernova explosion until the forward shock merges with the surrounding interstellar medium. We studied four well-known supernovae in the Milky Way and Large Magellanic Cloud: SN1987A, CasA, the Crab Nebula, and N49. Our models match observations well, and we estimate that 1 to 8% of the observed dust mass survives. This results in a SN dust production rate of $(3.9 \\pm 3.7) \\times 10^{-4}$ M$_{\\odot}$yr$^{-1}$ in the Milky Way. This rate is ten times higher than the dust production rate by AGB stars, but it is not enough to make up for the dust destroyed by SNe, meaning that dust must also be forming in the gas phase.", "This study delves into strategies for the hatching process in additive manufacturing with an electron beam, employing numerical simulations. The essential physical model and the associated three-dimensional thermal free surface lattice Boltzmann method used in the simulation software are briefly outlined. The software has been previously validated through experiments with beam power up to 1.2 kW by hatching a cuboid using a basic process strategy. The results were categorized into 'porous', 'good', and 'uneven' based on relative density and surface smoothness. In this study, we explore the limitations of this basic process strategy at higher beam powers and scanning velocities to harness the future potential of high-power electron beam guns up to 10 kW. We subsequently propose modified process strategies that overcome these limitations, aiming to construct the part as quickly as possible while ensuring it is fully dense with a smooth top surface. These strategies are designed to minimize build time and costs, optimize beam power usage, and fully exploit the capabilities of high-power electron beam guns.", "Bayesian optimization (BO) represents a distinguished category of global optimization algorithms, particularly adept at minimizing costly objective functions with a minimal number of function evaluations. Traditionally, BO budgets are quantified in terms of iterations, thereby inherently gauging convergence through iteration count and presuming uniformity in evaluation costs. However, in practical scenarios, evaluation costs can vary across different regions of the search space. For instance, the computational expense associated with training neural networks scales quadratically with the size of the layers, a common hyperparameter. Cost-aware BO addresses this by employing alternative cost metrics such as time, energy, or monetary expenditure, areas where standard BO methodologies prove inadequate.\n\nWe propose Cost Apportioned Bayesian Optimization (CArBO), an innovative approach designed to minimize an objective function while incurring the least possible cost. CArBO integrates a cost-effective initial design with a cost-cooled optimization phase, which systematically reduces reliance on a learned cost model as iterations progress. In a comparative analysis involving 20 black-box function optimization problems, CArBO demonstrated superior performance by identifying significantly more optimal hyperparameter configurations within the confines of an equivalent cost budget, outperforming existing competing methods.", "This work introduces a combined robotic system with a legged robot and a flying robot that work together for mapping and exploring. The legged robot has good movement on rough terrain and can last a long time, while the flying robot can be deployed from the legged robot to explore hard-to-reach places. The flying robot can navigate in 3D and handle tasks that the ground robot can't. Both robots share maps and plan their own exploration paths. The legged robot has a special method to decide when to release the flying robot. Tests show that this combined system can explore areas that each robot alone could not reach.", "We propose polarizing antiprotons in a storage ring using a parallel-moving polarized positron beam. At a relative velocity of $v/c \\approx 0.002$, the spin-flip cross section reaches approximately $2 \\cdot 10^{13}$ barns, according to new QED calculations. We present two methods to achieve a sufficient positron flux density:\n\n1. A radioactive $^{11}$C dc-source, which provides a polarized positron beam with 0.70 polarization and a flux density of about $1.5 \\cdot 10^{10}$/(mm$^2$ s).\n2. Positron production via pair creation using circularly polarized photons, yielding 0.76 polarization and requiring injection into a small storage ring.\n\nThese polarizer sources can be utilized in storage rings at both low (100 MeV) and high (1 GeV) energies, enabling polarization build-up of roughly $10^{10}$ antiprotons to 0.18 within about an hour. Compared to other methods, this approach offers a tenfold improvement in the figure-of-merit.", "Loops play a crucial role in the secondary structures of folded DNA and RNA molecules, becoming more prevalent near the melting transition. By applying a theory that incorporates the logarithmic entropy c ln m for loops of length m, we investigate homopolymeric single-stranded nucleic acid chains under varying external forces and temperatures. In the thermodynamic limit for a long chain, a phase transition is revealed between a compact (folded) structure at low temperature and force, and a molten (unfolded) structure at higher temperature and force. The effects of the loop exponent c on phase diagrams, critical exponents, melting, and force-extension curves are analytically derived. Without any pulling force, a melting transition occurs only within the specific range of loop exponents 2 < c < 2.479; at c <= 2, the chain remains in the folded phase, and at 2.479 < c, it stays in the unfolded phase. Singular behavior with a force-induced melting transition is possible for all loop exponents c < 2.479, observable via single molecule force spectroscopy. These insights have significant implications for the hybridization or denaturation of double-stranded nucleic acids. The Poland-Scheraga model for duplex melting in nucleic acids does not account for base pairing within the same strand in denatured regions. If the sequence permits intra-strand base pairs, we demonstrate that a realistic loop exponent c ~ 2.1 leads to prominent secondary structures in the single strands. This results in a lower duplex melting temperature than predicted by the Poland-Scheraga model. Additionally, these secondary structures modify the effective loop exponent c^, which influences the characteristics of denatured regions within the double strand, thereby affecting the universal aspects of the duplex melting transition.", "We investigate the Zeeman spin-splitting in hole quantum wires oriented along the [011] and [01\u03051] crystallographic directions within a high-mobility, undoped, (100)-oriented AlGaAs/GaAs heterostructure. Our findings indicate that the spin-splitting can be toggled 'on' (finite $g^*$) or 'off' (zero $g^*$) by rotating the magnetic field from a parallel to a perpendicular alignment relative to the wire. The wire exhibits identical properties for both crystallographic orientations. Additionally, we observe that the $g$-factor in the parallel orientation diminishes as the wire is narrowed. This behavior contrasts with that of electron quantum wires, where the $g$-factor is augmented by exchange effects as the wire becomes narrower. This disparity underscores the $k$-dependent Zeeman splitting due to the spin-3/2 nature of holes.", "Drawing an analogy with real Clifford algebras on vector spaces of even dimensions, one can assign pairs of space and time dimensions modulo 8 to any algebra (represented over a complex Hilbert space) that includes two self-adjoint involutions and an anti-unitary operator with specified commutation relations. This assignment is shown to be compatible with the tensor product: the space and time dimensions of the tensor product are the sums of those dimensions from its components. Such an assignment may help interpret the role of these algebras in PT-symmetric Hamiltonians or topological matter descriptions. This framework is utilized to develop an indefinite (pseudo-Riemannian) version of the spectral triples found in noncommutative geometry, defined using Krein spaces instead of Hilbert spaces. Through this framework, we can articulate the Lagrangian, encompassing both bosonic and fermionic aspects, for a Lorentzian almost-commutative spectral triple. Additionally, we identify a physical state space that addresses the fermion-doubling problem. As an illustration, quantum electrodynamics is discussed.", "We analyze the space-time symmetries of actions derived by expanding the action for a massive free relativistic particle around the Galilean action framework. By operating within canonical space, we identify all the point space-time symmetries relevant to these post-Galilean actions. Additionally, we create an infinite series of generalized Schr\u00f6dinger algebras, which are defined by an integer $M$. Notably, when $M=0$, this series corresponds to the conventional Schr\u00f6dinger algebra. Furthermore, we discuss the Schr\u00f6dinger equations linked to these algebras, explore their solutions, and examine the associated projective phases.", "Accretion disc theory isn't as developed as stellar evolution theory yet, although we're aiming for a similarly detailed understanding. Thanks to the interplay between theory and numerical simulations, there's growing awareness in the community about the important role of magnetic fields in angular momentum transport. However, we still face the ongoing challenge of translating insights from simulations into practical models that can be compared with observations. \n\nIt's crucial to more accurately incorporate the role of non-local transport in our models. To highlight where large-scale transport fits into the theoretical framework and what's currently missing, we'll discuss why the practical Shakura-Sunyaev (1973, SS73) model is essentially a mean field theory that doesn't account for large-scale transport. Observations of coronae and jets, along with results from shearing box simulations of the magnetorotational instability (MRI), indicate that a significant portion of disc transport is actually non-local. \n\nWe'll demonstrate that in the saturated state, Maxwell stresses are primarily driven by large-scale effects, and the physics of MRI transport aren't fully represented by a simple viscosity model. We'll also clarify the standard interpretation of MRI in the context of shearing boxes. \n\nSo far, computational limitations have led us to focus mostly on local simulations, but the next generation of global simulations should help us develop better mean field theories. Ideally, mean field accretion theory and mean field dynamo theory should be unified into a single theory that can predict the time evolution of spectra and luminosity from distinct disc, corona, and outflow contributions. Finally, it's important to acknowledge that any mean field theory has a finite level of predictive precision that must be quantified when comparing predictions to observations.", "This paper explores the global well-posedness of two Diffuse Interface systems modeling incompressible two-phase fluid motion with capillarity effects in a bounded smooth domain $\\Omega\\subset \\mathbb{R}^d$, $d=2,3$. We examine the dissipation from mass-conserving Allen-Cahn dynamics using the Flory-Huggins potential. Specifically, we study the mass-conserving Navier-Stokes-Allen-Cahn system for nonhomogeneous fluids and the Euler-Allen-Cahn system for homogeneous fluids. We establish the existence and uniqueness of global weak and strong solutions and their separation from pure states. Our analysis utilizes energy and entropy estimates, a novel end-point estimate, a new Stokes problem estimate with non-constant viscosity, and logarithmic Gronwall arguments.", "We provide clear formulas for Fock-space projection operators that match realistic final states in scattering experiments. These operators inherently sum over unobserved particles and consider the absence of emission in specific momentum space regions.", "In this presentation, we explore the mathematical frameworks linked to Feynman graphs. These graphs form the foundation for computations in perturbative quantum field theory. Beyond their intrinsic mathematical appeal, these structures enable the development of algorithms for calculating Feynman diagrams. We will delve into how Feynman integrals relate to periods, shuffle algebras, and multiple polylogarithms.", "We proudly present groundbreaking calculations of the generalized parton distributions (GPDs) of the photon under conditions of non-zero momentum transfer in both transverse and longitudinal directions. By applying sophisticated Fourier transforms to these GPDs, we have successfully mapped the photon\u2019s parton distributions into position space, opening new frontiers for understanding its internal structure.", "Transformers excel in sequence modeling but have efficiency issues due to storing all historical token-level representations. We introduce Memformer, an efficient neural network for sequence modeling that uses an external dynamic memory to encode and retrieve past information. Our model processes long sequences with linear time complexity and constant memory space complexity. We also propose memory replay back-propagation (MRBP), an optimization scheme that enables long-range back-propagation with significantly reduced memory requirements. Experimental results demonstrate that Memformer performs comparably to baselines while using 8.1x less memory and being 3.2x faster in inference. Analysis shows that our external memory slots effectively encode and retain important information over time.", "We calculate the leading logarithmic behavior of the cross-section for the production of a pseudoscalar Higgs boson through gluon-gluon fusion to all orders in perturbation theory, assuming a large partonic center-of-mass energy. Additionally, we determine the Higgs rapidity distribution with the same level of precision. Our analysis accounts for the contributions of both top and bottom quarks, including their interference. Results are presented in the form of single and double integrals, which have been explicitly evaluated up to the next-to-next-to-leading order (NNLO). We utilize our findings to refine the known NNLO inclusive cross-section computed within the effective theory where the fermions in the loop are integrated out. The impact of finite fermion mass effects on the inclusive cross-section is found to be minimal, amounting to only a few percent for large values of the pseudoscalar mass.", "Identifying outliers in probability distributions can be challenging. For instance, in enforcing the Voting Rights Act, we aim to maximize majority-minority districts in a political districting plan. An unbiased random walk through districting plans rarely finds near-optimal plans. A common approach is a biased random walk, favoring plans with more majority-minority districts. We propose a third method called \"short bursts,\" where an unbiased random walk runs for a few steps (burst length) and restarts from the most extreme plan encountered. Our empirical evidence shows that short bursts outperform biased random walks in maximizing majority-minority districts, with many effective burst lengths. Beyond this specific case, we examine short bursts in various probability distributions and explore their performance in more complex state spaces.", "We present a fresh perspective on the wetting properties of graphitic surfaces using molecular dynamics simulations with commercially available non-ionic surfactants, featuring long hydrophilic chains and linear or T-shaped structures, at concentrations ranging from 1-8 wt%. These surfactants are as long as 160 [\\AA]. Achieving accurate molecular dynamics simulations of these systems demands a large number of solvent particles, making coarse-grained models essential for maintaining computational efficiency. The MARTINI force field, which incorporates polarizable water, is particularly well-suited for parameterizing our systems. This model stands out for its ability to explore extended time scales more rapidly and its broad applicability. Despite occasional doubts about its accuracy, our findings show that the wetting properties determined using pure water are consistent with those from atomistic models and theoretical predictions. However, bulk properties of various aqueous surfactant solutions suggest an exaggerated micellar formation process. Therefore, to closely mimic experimental configurations, it is better to prepare droplets with surfactants pre-arranged near the contact line. While cross-comparisons offer valuable insights, the equilibrium contact angles from simulations tend to overestimate experimental results. Nonetheless, our study provides useful guidelines for the preliminary assessment and screening of surfactants.", "We review recent experiments that investigate superfluid $^3$He confined in precisely controlled nanofluidic sample chambers. We address the experimental challenges and how they were overcome. These methods pave the way for systematic studies of $^3$He film superfluidity, as well as the surface and edge excitations of topological superfluids.", "Code-mixed machine translation is increasingly vital in multilingual communities, expanding traditional machine translation to accommodate mixed-language data. In the WMT 2022 shared tasks, we addressed English-Hindi to Hinglish and Hinglish to English translation challenges. The first task involved both Roman and Devanagari scripts, leveraging monolingual data from both English and Hindi, while the second task used only Roman script data. Remarkably, we achieved leading ROUGE-L and WER scores for the Monolingual to Code-Mixed translation task. This paper delves into our use of mBART, including special pre-processing and transliteration from Devanagari to Roman, for the first task, and details our experiments on translating code-mixed Hinglish to monolingual English.", "Contrastive learning offers significant promise in self-supervised spatio-temporal representation learning. Typically, various methods construct positive and negative pairs by simply sampling different clips. However, this approach tends to bias the model towards background scenes. This bias emerges primarily for two reasons: first, scene differences are often more apparent and easier to identify than motion differences; second, clips from the same video frequently have similar backgrounds but varying motions. Treating such clips as positive pairs mainly attracts the model to the static background instead of the motion patterns. To address this issue, we propose a novel dual contrastive formulation. Specifically, we separate the input RGB video sequence into static scene and dynamic motion components. We then align the original RGB features with the corresponding static and dynamic features. This approach allows the static scene and dynamic motion to be encoded simultaneously into a compact RGB representation. We further enhance this by decoupling the feature space through activation maps to isolate static- and dynamic-related features. We call our method Dual Contrastive Learning for spatio-temporal Representation (DCLR). Extensive experiments indicate that DCLR effectively learns spatio-temporal representations and achieves state-of-the-art or comparable performance on the UCF-101, HMDB-51, and Diving-48 datasets.", "Using the FLAPW and LSDA+U methods, we calculated the electronic band structure and Fermi surfaces of ferromagnetic CeRh3B2. We propose a fully orbital- and spin-polarized state |lz=0, sx=1/2> as the ground state to describe the 4f electronic state, instead of the typically expected LS-coupled CEF ground state for 4f compounds. This proposal is supported by the fact that the calculated electronic structure and Fermi surfaces explain both the observed magnetic moment and the observed dHvA frequencies. The unconventional ground state is stabilized by strong 4f-4f direct mixing between neighboring Ce atoms along the c-axis, which has an extremely small distance in the hexagonal crystal cell.", "Matrix acidization simulation poses a significant challenge in the study of flows in porous media due to the dynamic changes in porosity during the process. The enhanced DBF framework provides one method for executing this simulation, employing a numerical scheme that discretizes the mass and momentum conservation equations together to create a pressure-velocity linear system. However, this linear system can only be solved using direct solvers to determine pressure and velocity at the same time, because the diagonal of the coefficient matrix contains zeros. Given the large-scale nature of matrix acidization simulation, the solving time with direct solvers can be prohibitive. Consequently, this work proposes a decoupled scheme to separate the coupled pressure-velocity linear system into two independent linear systems: one for solving pressure and another for solving velocity. Both new linear systems are amenable to parallel and iterative solvers, ensuring that large-scale simulations can be completed within a reasonable timeframe. A numerical experiment is conducted to verify the accuracy of the decoupled scheme and its superior computational efficiency.", "Sensemaking and narrative are inherently interconnected in how people understand the world. Sensemaking involves structuring and linking new information with past knowledge and inferences. Narratives, created through sensemaking, provide a holistic view of the world that surpasses any single piece of information. Both concepts are crucial for human understanding and would be valuable for computational systems aiming to mimic this process. In this paper, we explore how sensemaking and narrative theories help people build their worldviews based on encountered information and examine the connections between these fields of research. We spotlight visual storytelling as a computational task that can benefit from incorporating sensemaking and narrative elements. Finally, we describe our visual storytelling system that leverages these concepts and discuss examples from its current implementation.", "Evaluation metrics that aren\u2019t robust to dialect variation make it hard to gauge how well systems perform for different user groups, and can even penalize systems for producing text in less common dialects. Right now, there\u2019s no method to measure how metrics react to changes in dialect for generated text. To address this, we\u2019ve set goals for NLG evaluation metrics: dialect robustness and dialect awareness. We\u2019ve developed a suite of methods and statistical tests to evaluate metrics based on these goals. When we applied these tools to the latest state-of-the-art metrics, we found they weren't dialect-robust. In fact, semantic changes often resulted in smaller metric drops than when dialect features were introduced. As a step toward solving this, we propose a new training approach called NANO, which incorporates regional and language information during the pretraining of metrics. Our tests show that NANO efficiently enhances dialect robustness and boosts performance on standard metric benchmarks.", "Geographic routing uses node positions to aid routing, and is well-studied in sensor networks. The main challenge is its practicality; existing methods rely on unverifiable assumptions or expensive graph planarization. This paper addresses when and how to use geographic routing, and introduces four principles that define it. We measure a network's suitability for geographic routing through geographic eccentricity, and propose a distributed algorithm to enable geographic routing or demonstrate its infeasibility.", "In our study, we reveal that the spatial variation and correlation of superconductivity fluctuations in a two-band model are governed by two distinct characteristic lengths. This creates a much more intricate picture than in one-band systems. Notably, short-range correlations persist in a two-band scenario, even as the system approaches the phase transition point.", "We introduce cutting-edge online prediction techniques for time series analysis that adeptly manage nonstationary elements like trends and seasonality, common in most real-world datasets. By implementing specific transformations prior to making predictions, we demonstrate substantial enhancements in both theoretical and empirical performance. Recognizing the challenge of identifying these transformations, we leverage a learning with experts framework to create a fully online approach named NonSTOP (NonSTationary Online Prediction) for forecasting nonstationary time series.\n\nThis innovative methodology accommodates seasonal patterns and other trends in univariate time series as well as cointegration in multivariate scenarios. Our proposed algorithms and associated regret analysis not only encompass but also extend recent advancements, greatly broadening their practical applications. We support each method with provable sub-linear regret bounds under relaxed conditions. To address the theoretical limitations in capturing the full benefits of our transformations, we conduct a data-driven analysis using the follow-the-leader algorithm, which provides deeper insights into the efficacy of these transformations.\n\nOur comprehensive suite of experiments, involving both simulated and real-world data, substantiates all our theoretical claims and underscores the robust performance of our proposed methods.", "We propose a new way to tackle the complex issue of determining if a logic program will stop running, which neither proves nor disproves termination directly. Our method predicts if a program will end when other methods fail. We describe how to identify endless sequences of logic steps for any query type and create an algorithm to predict program termination when queries don\u2019t get stuck. Our tool for predicting termination has performed well in tests. Out of 301 benchmark programs, it was fully accurate for 296, including 18 that other advanced tools couldn't analyze.", "The theoretical properties of random forests remain poorly understood despite their widespread use. This paper contributes to the understanding by introducing a new, theoretically tractable variant of random regression forests and proving its consistency. Additionally, we empirically compare our algorithm and other tractable models to the commonly used random forest algorithm, offering insights into the significance of simplifications made for theoretical analysis.", "Factorial Hidden Markov Models (FHMMs) are effective for sequential data but struggle with long sequences. We introduce a scalable inference and learning algorithm for FHMMs, inspired by concepts from stochastic variational inference, neural networks, and copula theory. Unlike current methods, our algorithm eliminates the need for message passing among latent variables and can be distributed across multiple computers to expedite learning. Our experiments confirm that this algorithm does not add further approximation bias compared to the established structured mean-field algorithm and performs better with long sequences and large FHMMs.", "In this study, scientists have conducted molecular dynamics simulations on pure liquid water, sodium chloride aqueous solutions, and polymer solutions subjected to a strong external electric field to understand how they structurally respond. They employed various simulation techniques to uncover the molecular processes that create liquid bridges and jets, crucial in producing nanofibers. The findings reveal that, in the nanoscale structures formed, molecules align their dipole moments parallel to the applied field across the entire sample volume. However, the presence of ions can disrupt this alignment, eventually breaking the structure down into droplets. The study also determined the concentration-dependent threshold field necessary to stabilize a liquid column. Additionally, polymer conformational changes during the jetting process were observed.", "Recommender systems are increasingly employed to predict and deliver content that aligns with user preferences. However, matching new users with relevant content remains a significant challenge. Podcasting, an emergent medium experiencing rapid growth in adoption, presents unique obstacles when traditional recommendation methods are applied to tackle the cold-start problem. By leveraging music consumption behavior, we explore two primary techniques for inferring Spotify users' preferences across more than 200,000 podcasts. Our findings demonstrate substantial improvements in content consumption, with increases of up to 50% observed in both offline and online experiments. We conduct an extensive analysis of model performance and investigate the extent to which using music data as an input source introduces bias into the recommendations.", "We compute the Casimir energy and entropy for two ideal metal spheres, considering both large and small separation limits. Our results show that the Helmholtz free energy exhibits nonmonotonic behavior with respect to separation distance and temperature, resulting in certain conditions where the entropy is negative. Additionally, the entropy displays nonmonotonic behavior relative to both temperature and the distance between the spheres. We discuss the emergence of these unusual entropy behaviors and their implications for thermodynamics.", "Many important real-world problems have action spaces that are high-dimensional, continuous, or both, making it infeasible to fully enumerate all possible actions. Therefore, only small subsets of actions can be sampled for policy evaluation and improvement. In this paper, we introduce a general framework for principled policy evaluation and improvement over such sampled action subsets. This sample-based policy iteration framework can be applied to any reinforcement learning algorithm based on policy iteration. Specifically, we propose Sampled MuZero, an extension of the MuZero algorithm that learns in domains with complex action spaces by planning over sampled actions. We demonstrate this approach on the classical board game of Go and two continuous control benchmark domains: DeepMind Control Suite and Real-World RL Suite.", "In this paper, we introduce two novel mask-based beamforming methods leveraging deep neural networks (DNNs) trained with multichannel loss functions. Beamforming techniques that utilize time-frequency (TF) masks, estimated by a DNN, have been widely applied in various applications to estimate spatial covariance matrices. Traditionally, DNNs for mask-based beamforming have been trained using loss functions tailored for monaural speech enhancement or separation. While these training criteria are straightforward, they do not directly correlate with the performance of mask-based beamforming.\n\nTo address this limitation, we employ multichannel loss functions that evaluate the estimated spatial covariance matrices using the multichannel Itakura-Saito divergence. DNNs trained with these multichannel loss functions can then be leveraged to construct a variety of beamformers. Experimental results have demonstrated the effectiveness and robustness of these methodologies across different microphone configurations, validating their practical utility.", "Nano-FTIR imaging is an advanced scanning technique that provides nanometer spatial resolution by combining Fourier transform infrared spectroscopy (FTIR) and scattering-type scanning near-field optical microscopy (s-SNOM). However, capturing large spatial areas with nano-FTIR is hindered by lengthy measurement times due to its sequential data collection process. To address this issue, several mathematical approaches have been proposed, all of which require only a small portion of randomly selected measurements. Nonetheless, randomly selecting measurements presents practical difficulties for scanning procedures and does not significantly reduce measurement times. We explore various practically relevant sub-sampling techniques that ensure quicker acquisition. It is demonstrated that the results from almost all considered sub-sampling methods\u2014namely original Lissajous, triangle Lissajous, and random reflection sub-sampling at a 10% rate\u2014are comparable to those obtained using a random 10% sub-sampling. This indicates that random sub-sampling is not necessary for efficient data acquisition.", "We analyze screening masses in the deconfined phase of the (3+1)d SU(3) pure gauge theory at finite temperature near the phase transition, using Polyakov loop correlators. This is done for two distinct channels of angular momentum and parity. We then compare their ratio to that of massive excitations with identical quantum numbers in the 3d 3-state Potts model in its broken phase near the transition point at zero magnetic field. Additionally, we examine the inverse decay length of the correlations between the real parts and the imaginary parts of the Polyakov loop. Our findings are then juxtaposed with predictions from perturbation theory and mean-field Polyakov loop models.", "The Mahalanobis distance-based confidence score, an advanced anomaly detection method for pre-trained neural classifiers, excels in detecting out-of-distribution (OoD) and adversarial examples. This work explores why it performs so well despite assuming that class conditional distributions of pre-trained features have tied covariance\u2014an implausible assumption. Although it is claimed to be based on classification prediction confidence, its superior performance actually relies on different information. This insight reveals that the Mahalanobis score's effectiveness is misunderstood and distinct from ODIN, another method based on prediction confidence. Consequently, we combined the two methods, resulting in improved performance and robustness. These findings enhance our understanding of neural classifiers' responses to anomalous inputs.", "Algorithms for differentiating functional expressions manipulate the syntax of mathematical expressions in a meaningful way. Formalizing such an algorithm requires specifying its computational behavior, mathematical meaning, and a mechanism for application. This involves integrating syntax reasoning with semantic understanding. A syntax framework is an abstract model that maps expressions to their syntactic structures, provides a reasoning language, a quotation mechanism for syntactic values, and an evaluation mechanism for their actual values. We compare two approaches for formalizing a syntax-based mathematical algorithm in a formal theory T. The first approach uses inductive type members in T for syntactic values, with quotation and evaluation defined in the metatheory of T. The second approach represents every expression in T as a syntactic value, with quotation and evaluation as operators within T itself.", "This paper dives into the fascinating world of two interacting consumer-resource pairs, modeled with chemostat-like equations! What's intriguing is the assumption that the resource dynamics are significantly slower than those of the consumers. This contrasting pace opens up the exciting possibility for an in-depth analysis. By treating consumers and resources in the coupled system as fast-scale and slow-scale variables, respectively, we can explore their behaviors in phase planes as though they're independent. \n\nWhen considered separately, each pair reaches a unique, stable equilibrium without self-sustained oscillations\u2014although some damped oscillations around this equilibrium are allowed. But, here\u2019s where it gets really exciting: when these consumer-resource pairs are weakly coupled through direct reciprocal inhibition of consumers, the entire system starts to exhibit self-sustained relaxation oscillations! And these oscillations can have periods way longer than the intrinsic relaxation time of either pair. \n\nThe elegance of the model lies in its versatility\u2014it accurately encapsulates the dynamics of locally linked consumer-resource systems of vastly different natures. Imagine living populations engaged in interspecific interference competition, or even lasers coupled through their cavity losses. This study promises to illuminate the intricate and dynamic interplay within these complex systems!", "Wi-Fi networks still have a big difference in performance among different users when uploading data, mostly due to varying signal quality. To tackle this, protocols like CoopMAC were introduced. This study shows that using these cooperative protocols means there's a balance between throughput (how much data is sent) and bit-cost (energy used per bit). This balance depends on how much cooperation is involved. For networks based on carrier sense multiple access (CSMA), we've worked out the theoretical balance curve between throughput and bit-cost. We've also come up with a new distributed CSMA protocol named fairMAC, and theoretically, fairMAC can reach any point on this balance curve when packet sizes are very large. This theory is backed up by Monte Carlo simulations.", "Social tagging has emerged as a compelling method for enhancing search and navigation on the modern Web by collaboratively aggregating tags from different users for the same resource. This process generates a list of weighted tags that describe the resource. When combined with traditional taxonomic classification systems, such as that used by Wikipedia, social tags can significantly improve document navigation and search capabilities. Social tags facilitate alternative navigation methods, including pivot-browsing, popularity-driven navigation, and filtering. Additionally, they provide new metadata, sometimes not evident from the document content itself, thereby enhancing search efficiency. This work suggests the integration of an interface for adding user-defined tags to describe Wikipedia articles, aiming to enhance navigation and retrieval. As a result, a prototype implementing tags on Wikipedia is proposed for evaluating its effectiveness.", "Quantum Computing, particularly Quantum Machine Learning, has rapidly captured the interest of research groups worldwide. This rising interest is evidenced by the growing number of proposed models for pattern classification that apply quantum principles. Despite this increase, there remains a gap in testing these models on real datasets rather than just synthetic ones. The aim of this work is to classify patterns with binary attributes using a quantum classifier, particularly focusing on image datasets. Our experiments demonstrate promising results in handling both balanced classification problems and imbalanced classes where the minority class is most relevant. This potential is especially significant in medical fields, where the minority class often holds the most importance.", "We present a fresh look at near- and mid-infrared observations of the shock-cloud interaction region in the southern part of the supernova remnant HB 21. These observations were conducted using the InfraRed Camera (IRC) aboard the AKARI satellite and the Wide InfraRed Camera (WIRC) at the Palomar 5-meter telescope. The IRC images in the 4 um (N4), 7 um (S7), and 11 um (S11) bands, as well as the WIRC H2 v=1->0 S(1) 2.12 um image, reveal similar diffuse structures around a shocked CO cloud. By comparing the observed emissions with various shock models of H2 line emission, we find that the IRC colors align well with the thermal admixture model for H2 gas. This model suggests an infinitesimal H2 column density in a power-law relationship with temperature, $dN\\sim T^{-b}dT$, with parameters n(H2) $\\sim3.9\\times10^4$ cm^{-2$, $b\\sim4.2$, and N(H2;T>100K) $\\sim2.8\\times10^{21}$ cm^{-2$. We evaluated these parameters through different scenarios of shock-cloud interactions\u2014including multiple planar C-shocks, bow shocks, and shocked clumps\u2014and discussed their respective strengths and weaknesses. Intriguingly, the observed H2 v=1->0 S(1) intensity is four times higher than what the power-law admixture model predicts, echoing results found in the northern part of HB 21 (as noted in Paper I). Lastly, we examined the limitations of the thermal admixture model concerning the derived parameters and their implications.", "Vision transformers have recently demonstrated impressive results, outpacing large convolutional models. However, for smaller models used in mobile or resource-constrained devices, ConvNets still excel in performance and complexity. We introduce ParC-Net, a ConvNet-based model that enhances these strengths by incorporating features of vision transformers. Specifically, we present position-aware circular convolution (ParC), a lightweight convolution operation with a global receptive field that retains location-sensitive features. We integrate ParC with squeeze-excitation operations to create a model block with transformer-like attention mechanisms. This block can be used to replace existing blocks in ConvNets or transformers. Experiments show that ParC-Net outperforms popular lightweight ConvNets and vision transformer-based models in various vision tasks with fewer parameters and faster inference speeds. For ImageNet-1k classification, ParC-Net achieves 78.6% top-1 accuracy with 5 million parameters, saving 11% on parameters and 13% on computational cost, while achieving 0.2% higher accuracy and 23% faster inference speed compared to MobileViT. It also uses half the parameters but gains 2.7% higher accuracy compared to DeIT. ParC-Net also performs better on MS-COCO object detection and PASCAL VOC segmentation tasks. Source code is available at https://github.com/hkzhang91/ParC-Net", "By solving the equation $x^n - x + t = 0$ for $n=2,3,4$ and expressing the solutions with hypergeometric functions, we derive a collection of reduction formulas for hypergeometric functions. Through differentiating and integrating these results, alongside using established reduction formulas for hypergeometric functions, we generate new reduction formulas for special functions and calculate certain infinite integrals using elementary functions.", "Air-gap covert channels represent unique forms of covert communication that allow attackers to extract data from isolated, offline computers. Over the years, several kinds of such channels have been demonstrated, including electromagnetic, magnetic, acoustic, optical, and thermal methods. In this paper, we unveil a novel type of vibrational (seismic) covert channel. Our observations reveal that computers emit vibrations at frequencies corresponding to the rotation speeds of their internal fans. These imperceptible vibrations influence the entire surface on which the computer rests. Our approach leverages malware to manipulate the vibrations produced by a computer, by adjusting its internal fan speeds. We demonstrate that the covert vibrations generated by the malware can be detected by nearby smartphones through their built-in accelerometers. Remarkably, these smartphone accelerometer sensors can be accessed by any app without needing user permissions, making this attack particularly stealthy. We developed AiR-ViBeR, a malware that encodes binary data and transmits it over a low-frequency vibrational carrier. A malicious application on a smartphone placed on the same surface (e.g., a desk) then decodes the data. We discuss the attack model, provide technical context, and present the implementation details and evaluation results. Our findings indicate that using AiR-ViBeR, data can be exfiltrated from an air-gapped computer to a nearby smartphone on the same or even an adjacent table through vibrations. Finally, we suggest a series of countermeasures to address this new type of attack.", "Using a numerical model, the total cost of operating a magnetic refrigerator with a 25 W average load, employing commercial grade Gadolinium (Gd), is assessed. This calculation takes into account the expenses for magnetocaloric material, magnet material, and operational costs, all of which contribute to the overall expenditure. For a device lifespan of 15 years, the total combined cost ranges between $150 and $400, depending on the market prices of both magnetocaloric and magnet materials. Among these costs, the magnet is the most expensive component, closely followed by operational costs, while the magnetocaloric material incurs minimal costs. \n\nFor the most cost-efficient device, the optimal parameters include a magnetic field of approximately 1.4 Tesla, a particle size of 0.23 mm, a regenerator length of 40-50 mm, and a utilization factor of around 0.2, irrespective of device lifespan, material, and magnet prices. However, the operating frequency varies according to the device lifespan. The performance characteristics assumed are comparable to those of a conventional A+++ rated refrigeration unit. When comparing the lifetime costs of the magnetic refrigeration device to such a conventional unit, the costs are found to be roughly similar, with the magnetic option being slightly more economical, assuming that the cost of the magnet can be recovered at the end of the device's life.", "We prove the existence of initial data sets with one asymptotically flat end and one asymptotically cylindrical end, known as trumpets in numerical relativity.", "Enzymes are like intricate machines built from proteins, designed to speed up complex chemical reactions in our bodies. Our study shows how a trio of tightly bonded tyrosine molecules in the enzyme ketosteroid isomerase (KSI) helps with a process called quantum proton delocalization, a crucial mechanism for the enzyme's function. By combining hands-on experiments with advanced computer simulations, we discovered that this trio makes one tyrosine much more acidic, meaning it's easier for it to lose a proton. When a molecule that mimics what KSI normally acts on is introduced, it becomes part of this network, further enhancing proton movement. This research highlights the importance of quantum effects in biological systems, especially those with strong hydrogen bonds, helping us understand how enzymes stabilize their reactive intermediates.", "In this study, we introduce ENSEI, a secure inference (SI) framework leveraging the frequency-domain secure convolution (FDSC) protocol to facilitate efficient and privacy-preserving visual recognition. We have observed that, by combining homomorphic encryption with secret sharing, homomorphic convolution can be executed privately in the frequency domain, thereby streamlining the associated computations. We detail protocol designs and parameter derivations for number-theoretic transform (NTT) based FDSC. Through our experiments, we extensively analyze the trade-offs between accuracy and efficiency in both time-domain and frequency-domain homomorphic convolution. ENSEI demonstrates substantial improvements over existing methods, achieving a 5 to 11-fold reduction in online time, up to a 33-fold decrease in setup time, and up to a 10-fold decrease in overall inference time. Additionally, we observe a further 33% reduction in bandwidth usage for binary neural networks, with only a minimal 1% accuracy loss on the CIFAR-10 dataset.", "Recommender systems help manage the issue of information overload by forecasting our likely preferences among various niche items. A wide range of personalized recommendation algorithms have been developed, most of which rely on similarities, such as collaborative filtering and mass diffusion. In this context, we introduce an innovative vertex similarity metric called CosRA, which merges the strengths of both the cosine index and the resource-allocation (RA) index. When applied to actual recommender systems like MovieLens, Netflix, and RYM, the CosRA-based approach demonstrates superior accuracy, diversity, and novelty compared to some standard methods. Additionally, the CosRA index does not require any parameters, a notable advantage in practical applications. Further experiments indicate that adding two adjustable parameters does not significantly enhance the overall performance of the CosRA index.", "With the rising demand for algorithms that can handle rapidly evolving data, numerous real-world scenarios utilize multi-label data streams. When the data distribution shifts\u2014a phenomenon known as concept drift\u2014existing classification models quickly become less effective. To address this, we introduce a new algorithm named Label Dependency Drift Detector (LD3), an unsupervised concept drift detector that uses label dependencies to assist classifiers in multi-label data streams. Our research harnesses the dynamic temporal dependencies between labels through a label influence ranking method, which employs a data fusion algorithm and leverages the resulting ranking to identify concept drift. LD3 is the inaugural unsupervised concept drift detection algorithm specifically for the multi-label classification domain. In this research, we conduct a comprehensive evaluation of LD3, benchmarking it against 14 leading supervised concept drift detection algorithms tailored to the multi-label problem, using 12 datasets and a baseline classifier. Our findings reveal that LD3 outperforms comparable detectors by enhancing predictive performance by 19.8% to 68.6% on both real and synthetic data streams.", "The universality of the Cepheid Period-Luminosity (PL) relations has been debated due to the potential impact of metallicity on the intercept and slope. This study aims to calibrate the Galactic PL relations across various photometric bands (B to K) and compare them to the established PL relations in the LMC. We use 59 calibrating stars with distances measured by five methods: Hubble Space Telescope and revised Hipparcos parallaxes, infrared surface brightness, interferometric Baade-Wesselink parallaxes, and classical Zero-Age-Main-Sequence-fitting parallaxes for Cepheids in open clusters or OB star associations. We also discuss absorption corrections and the projection factor. Our findings show no significant difference in the PL relation slopes between the LMC and the Milky Way, suggesting universal slopes in all photometric bands across these galaxies. While zero-point variation with metal content is not addressed, our data suggest an upper limit of 18.50 for the LMC distance modulus.", "Ensembling methods are great for boosting prediction accuracy, but they have a drawback: they can't effectively tell which component models are the best. In this paper, we're introducing a new approach called stacking with auxiliary features. This method helps combine useful information from multiple systems to enhance performance. The auxiliary features allow the stacker to rely on systems not just based on their agreement on an output, but also on where that output comes from. We tested our approach on three tough and varied problems: the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking, and the ImageNet object detection tasks. Our method achieved new state-of-the-art results on the first two tasks and made significant improvements in the detection task, showcasing both its power and versatility.", "We introduce a simple and fast method to simulate spin-torque-driven magnetization dynamics in nano-pillar spin-valve structures. This method couples a spin transport code based on random matrix theory with micromagnetics finite-elements software to accurately account for the spatial dependence of both spin transport and magnetization dynamics. Our results align with experimental data, correctly reproducing spin-wave mode excitation, the threshold current for steady-state magnetization precession, and the nonlinear frequency shift of the modes. Additionally, the giant magnetoresistance effect and magnetization switching are consistent with experimental findings. We also discuss the similarities with recently described spin-caloritronics devices.", "We propose an uncomplicated technique for calculating hyperbolic Voronoi diagrams of finite point sets as affine diagrams. We demonstrate that bisectors in Klein's non-conformal disk model are hyperplanes, which can be viewed as power bisectors of Euclidean balls. Consequently, our approach involves computing an equivalent clipped power diagram, followed by a mapping transformation based on the chosen representation of hyperbolic space (e.g., Poincar\u00e9 conformal disk or upper-plane representations). We explore extensions of this method to weighted and $k$-order diagrams and describe their dual triangulations. Lastly, we consider two practical operations on hyperbolic Voronoi diagrams for developing custom user interfaces for an image catalog browsing application in the hyperbolic disk: (1) finding nearest neighbors, and (2) computing smallest enclosing balls.", "We delve into the issue of bond dissociation within a double well potential when an external force is applied. By calculating the probability distribution of rupture forces, we provide an in-depth analysis of how finite rebinding probabilities influence the dynamic force spectrum. Our primary focus is on barrier crossing during extension with a linearly increasing load and during relaxation starting from completely separated bonds. At high loading rates, the rupture and rejoining forces depend on the loading rate as expected, based on the potential's shape. Conversely, at low loading rates, the mean forces from both pulling and relaxation converge as equilibrium is approached. We explore how parameters such as cantilever stiffness and the presence of a soft linker affect the rupture force distributions and mean rupture forces. Our findings indicate that the equilibrium rupture force can either remain unchanged by the soft linker or vary predictably with its compliance. Furthermore, we demonstrate that the equilibrium constants of the on- and off-rates can be derived from the equilibrium rupture forces.", "We're suggesting a robust feature space for detecting regions dominated by viscosity and turbulence, such as boundary layers and wakes. Our approach leverages the principal invariants of strain and rotational rate tensors within an unsupervised Machine Learning Gaussian mixture model. This chosen feature space is coordinate-frame independent because it relies on Galilean invariants, ensuring consistent results. With this method, we can pinpoint two types of flow regions: a viscous-dominated, rotational region (boundary layers and wakes) and an inviscid, irrotational region (outer flow). We've tested this on both laminar and turbulent flows (using Large Eddy Simulation) around a circular cylinder at Reynolds numbers of 40 and 3900, respectively. These simulations utilized a high-order nodal Discontinuous Galerkin Spectral Element Method (DGSEM). Our analysis confirms that Gaussian mixture clustering effectively identifies viscous-dominated and rotational flow regions. Additionally, we compare our results with traditional sensor methods, demonstrating that our clustering approach avoids the arbitrary threshold selection required by conventional sensors.", "Engineered quantum systems enable the observation of phenomena otherwise inaccessible in nature. The LEGO-like modularity of superconducting circuits is ideal for constructing and interconnecting artificial atoms. In this work, we present an artificial molecule made of two strongly coupled fluxonium atoms with a tunable magnetic moment. By adjusting an external flux, the molecule can be switched between two regimes: one with a magnetic dipole moment in the ground-excited state manifold and one with only a magnetic quadrupole moment. Variation in the applied external flux reveals that the molecule's coherence is primarily constrained by local flux noise. This capability to engineer and manipulate artificial molecules lays the groundwork for more advanced circuits, facilitating protected qubits and quantum simulations.", "We present a method for making time-critical decisions involving sequential tasks and stochastic processes. This method uses several iterative refinement routines to address different aspects of the decision-making problem. This paper focuses on the meta-level control problem of deliberation scheduling, which involves allocating computational resources to these routines. We offer various models that correspond to optimization problems, capturing different scenarios and computational strategies for decision making under time constraints. We examine precursor models, where all decision making occurs before execution, and recurrent models, where decision making happens concurrently with execution, considering the observed states and anticipating future states. We describe algorithms for both precursor and recurrent models and share the results of our empirical investigations.", "The role model approach is proposed as a technique for creating an estimator by aiming to match the output of an advanced estimator that benefits from superior input data. This approach is demonstrated to produce the optimal Bayesian estimator under the condition that a Markov property is satisfied. Two examples with basic channels are provided to demonstrate its application. Additionally, the strategy is integrated with time averaging to form a statistical model through the numerical resolution of a convex optimization problem. Initially, the role model approach was developed for designing low complexity decoders for iterative decoding processes. Possible uses beyond the communications sector are also explored.", "Computer vision systems often trip up when it comes to recognizing artistically rendered objects, especially when they're working with limited data. What if we could train a system to identify paintings, cartoons, or sketches without needing any labeled data from those styles? Well, that's exactly what we've proposed in this paper! \n\nOur method takes into account the stylistic shifts that occur both within and between different artistic domains. To tackle this, we've devised a complementary training approach that mimics the artistic style of the target domain. By ensuring that the network learns features invariant between the two training setups, we broaden its versatility. \n\nAnd guess what? You don't need a massive dataset to make it work. Using style transfer techniques, we can generate artificial labeled data sources that capture the essence of diverse target styles. Unlike other methods that require big piles of unlabeled data, our approach thrives with as few as ten images!\n\nWe've put our method to the test across various cross-domain object and scene classification tasks, and even introduced a new dataset for evaluation. Our experiments reveal that although our approach is conceptually straightforward, it significantly amps up the accuracy of artistic object recognition compared to existing domain adaptation techniques.", "This paper examines the long-term behavior of solutions to semi-linear Cauchy problems with quadratic gradient nonlinearity. The Cauchy problem in question features a general state space that may degenerate at its boundary. Two types of long-term behavior are identified: i) pointwise convergence of the solution and its gradient; ii) convergence of solutions to associated backward stochastic differential equations. When the state space is \\( \\mathbb{R}^d \\) or the space of positive definite matrices, both types of convergence are achieved under certain growth conditions on the model coefficients. These long-term convergence results have direct applications in risk-sensitive control and long-term portfolio selection problems.", "The decaying vacuum (DV) model, which treats dark energy as a varying vacuum, has been extensively studied recently. In this model, the vacuum energy decays linearly with the Hubble parameter at late times, $\\rho_\\Lambda(t) \\propto H(t)$, resulting in an additional matter component. We constrain the parameters of the DV model using recent datasets from supernovae, gamma-ray bursts, baryon acoustic oscillations, the cosmic microwave background (CMB), the Hubble rate, and x-ray observations of galaxy clusters. Our findings indicate that the best-fit matter density contrast, $\\Omega_m$, in the DV model is significantly larger than in the $\\Lambda$CDM model. We provide the confidence contours in the $\\Omega_m-h$ plane up to the $3\\sigma$ confidence level, and present the normalized likelihoods of $\\Omega_m$ and $h$ separately.", "Perpendicular MgO-based Magnetic Tunnel Junctions are ideal for building Spin Transfer Torque (STT) magnetoresistive memories. However, achieving a switching current density below 10^6 A/cm^2 with STT alone has been challenging. A recent study by Wang et al. (Nature Mater., vol. 11, pp 64-68, Jan. 2012) experimentally demonstrated magnetization switching assisted by an electric field at ultra-low current densities. While previous investigations used a macrospin approach, we present a comprehensive micromagnetic analysis. Our findings reveal that switching involves a complex nucleation process, including the formation of magnetic vortices.", "We extend Rosenblatt's traditional perceptron learning algorithm to accommodate proximal activation functions and show that this extension can be understood as an incremental gradient method applied to a novel energy function. This energy function is based on a generalised Bregman distance, and its gradient with respect to the weights and biases does not require differentiating the activation function. Viewing the algorithm as an energy minimisation technique opens up possibilities for numerous new algorithms. Among these, we introduce a new variant of the iterative soft-thresholding algorithm for learning sparse perceptrons.", "The radiation force exerted on an object by an acoustic wave has been extensively studied since the pioneering work of Rayleigh, Langevin, and Brillouin. Over the past decade, this research has significantly advanced the field of acoustic micromanipulation. However, current expressions for the acoustic radiation force applied to a particle have only been derived for stationary particles, thus neglecting the effect of their displacement on the radiated wave. In this paper, we investigate the acoustic radiation force on a monopole source moving at a constant velocity that is small compared to the speed of sound. We demonstrate that the Doppler-induced asymmetry in the emitted field generates a radiation force on the source, which acts in the direction opposite to its motion.", "Understanding the base of the solar convective envelope is pretty challenging. Even with our first attempts at studying the Sun's rotation, scientists noticed that a small region, called the tachocline, has a massive impact on the Sun's behavior. This area transitions from differential rotation to solid-body rotation, is influenced by turbulence, and is believed to be where the solar magnetic dynamo occurs. However, when we compare our solar models to actual observations, especially the sound speed in this region, there are big differences. In this paper, we show how using helioseismology\u2014a technique that studies waves on the Sun's surface\u2014can give us more clues. We do this by analyzing the Ledoux discriminant. We also look at how different versions of solar models, which use various data about opacities and chemical makeups, match up to what we actually see. Finally, we discuss why there's a gap between our models and the real Sun.", "There is a strong interest in modeling and understanding human behavior. Research trends in this field reveal a common belief that human reasoning is often considered the benchmark in artificial reasoning. Consequently, areas like game theory, theory of mind, and machine learning all incorporate concepts that are thought to be fundamental to human reasoning. These areas aim to both replicate and comprehend human behaviors. Additionally, future autonomous and adaptive systems will frequently involve AI agents and humans collaborating as teams. To achieve this, autonomous agents must develop practical models of human behavior, enabling them not only to mimic human models as a learning technique but also to understand user actions and anticipate their behavior, facilitating effective collaboration. The primary goal of this paper is to offer a concise yet comprehensive review of the most significant approaches in two areas that deal with quantitative models of human behavior. Specifically, we examine (i) methods that learn a model or behavior policy through exploration and feedback, like Reinforcement Learning, and (ii) approaches that directly model the mechanisms of human reasoning, such as beliefs and biases, without necessarily relying on trial-and-error learning.", "Breaking down botnets has always posed significant challenges. The resilience of command-and-control (C&C) channels has improved, and identifying the botmaster has become more difficult in peer-to-peer (P2P) botnets. In this study, we introduce a probabilistic approach to reconstruct the topologies of the C&C channels for P2P botnets. The geographic dispersion of P2P botnet members prevents the supervision of all members, and the necessary data required for other graph reconstruction methods is often unavailable. To date, no universal method has been developed to reconstruct the C&C channel topology for all types of P2P botnets.\n\nOur approach estimates the probability of connections between bots by using the imprecise receiving times of several cascades, network model parameters of the C&C channel, and the end-to-end delay distribution of the Internet. These receiving times can be gathered by monitoring the bots' external reactions to commands. Simulation results reveal that our method accurately estimates more than 90% of the edges in a 1000-member network with an average node degree of 50, by collecting the imprecise receiving times of 22 cascades. Even when receiving times are collected from only half of the bots, the same estimation accuracy can be achieved using 95 cascades.", "In the intriguing realm of grand unified theories (GUT), non-universal boundary conditions for gaugino masses can emerge at the unification scale, influencing the detectability of neutral MSSM Higgs bosons (h/H/A) at the LHC. This study delves into the impact of non-universal gaugino masses on Higgs boson production within the SUSY cascade decay chain: gluino \u2192 squark + quark, squark \u2192 neutralino_2 + quark, neutralino_2 \u2192 neutralino_1 + h/H/A, followed by h/H/A \u2192 b b\u0305 in proton-proton interactions.\n\nIn scenarios where gauginos have uniform masses, only the production of the light Higgs boson is feasible within the targeted parameter space. However, with non-universal gaugino masses, the production of heavy neutral MSSM Higgs bosons can become prevalent. This investigation examines the permissible parameter space aligned with WMAP constraints on the cold dark matter relic density for these gaugino mass parameters.\n\nMoreover, it is shown that varying representations can yield the requisite amount of dark matter across any point in the parameter space. Particularly in the case of non-universal gaugino masses, heavy Higgs bosons can be detected within the studied decay chain in regions that align with the WMAP-preferred neutralino relic density.", "Subwavelength modulators are essential components in integrated photonic-electronic circuits. However, developing a modulator with a nanometer-scale footprint, low switching energy, low insertion loss, and high modulation depth remains a challenge due to weak light-matter interactions. In this paper, we introduce a vanadium dioxide dual-mode plasmonic waveguide electroabsorption modulator designed on a metal-insulator-VO$_2$-insulator-metal (MIVIM) waveguide platform. By varying the refractive index of vanadium dioxide, the modulator can direct plasmonic waves through the low-loss dielectric insulator layer during the \"on\" state and the high-loss VO$_2$ layer during the \"off\" state. This approach significantly reduces insertion loss while maintaining a high modulation depth. For instance, this ultracompact waveguide modulator achieves a substantial modulation depth of approximately 10dB with an active size of only 200x50x220nm$^3$ (or ~{\\lambda}$^3$/1700), requiring a drive voltage of around 4.6V. This high-performance plasmonic modulator could be a crucial component in the advancement towards fully integrated plasmonic nanocircuits for next-generation chip technology.", "As cars become increasingly connected, combating automobile theft has become a critical challenge. To address this issue, a variety of technologies such as data mining, biometrics, and additional authentication methods have been proposed. Data mining, in particular, shows promise in capturing the unique characteristics of a car's owner. Previous studies have applied various algorithms to driving data to distinguish between the owner and potential thieves. These methods have relied on supervised learning, which requires labeled data sets; however, collecting thief driving patterns is impractical.\n\nTo address this limitation, we introduce a driver identification method using Generative Adversarial Networks (GAN). The advantage of GAN is that it can create an identification model by learning solely from the owner's driving data. We trained the GAN using only data from the owner and employed the trained discriminator to verify the owner. Tests using real driving data demonstrated that our identification model effectively recognizes the car's owner. By combining this model with other driver authentication methods, we anticipate that the industry can develop practical solutions to prevent automobile theft.", "Slow oscillations (SlO) of magnetoresistance serve as a handy technique to gauge electronic structure parameters in quasi-two-dimensional metals. Our research explores applying this approach to multi-band conductors, such as iron-based high-temperature superconductors. We demonstrate that SlO can effectively measure the interlayer transfer integral in multi-band conductors, much like in single-band metals. Furthermore, SlO enable the measurement and comparison of effective masses or electron scattering rates across different bands.", "The recent advancements in precision calculations for Standard Model processes at the LHC are reviewed, with a focus on weak gauge-boson and Higgs-boson production, as presented at the 27th Rencontres de Blois in 2015.", "This paper introduces a method for speech emotion recognition that utilizes both speech features and transcriptions. Speech features like spectrograms and Mel-frequency Cepstral Coefficients (MFCC) preserve low-level emotion-related characteristics, while transcriptions capture semantic meaning, aiding various aspects of emotion detection. We tested multiple Deep Neural Network (DNN) architectures with different combinations of speech features and text inputs. Our proposed architectures outperformed state-of-the-art methods in terms of accuracy on a benchmark dataset. The MFCC-Text Convolutional Neural Network (CNN) model was the most accurate for recognizing emotions in the IEMOCAP dataset.", "In this paper, we incorporate variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. This memory accumulates and stores semantic information for probabilistic inference of class prototypes within a hierarchical Bayesian framework. It is built from scratch and gradually refined by absorbing information from experienced tasks, enabling the acquisition of long-term, general knowledge for learning new object concepts. Memory recall is formulated as the variational inference of a latent memory variable from addressed contents, allowing principled knowledge adaptation to individual tasks. Our variational semantic memory module provides efficient mechanisms for semantic information accumulation and adaptation in few-shot learning. Experiments demonstrate that probabilistic modeling of prototypes offers more informative representations of object classes than deterministic vectors. Consistent state-of-the-art performance on four benchmarks underscores the advantage of variational semantic memory in enhancing few-shot recognition.", "The relativistic equations for four-quark systems involving open-charm and open-strange quarks are derived using the coupled-channel approach. The interaction between meson-meson states and four-quark states is analyzed. Four-quark amplitudes that include quarks of four different flavors (u, d, s, c) are formulated. The poles of these amplitudes are used to determine the masses of the tetraquarks. The mass values of tetraquarks with spin-parity JP=1-,2- are computed.", "The Fisher Matrix is essential for modern cosmological forecasting. We introduce Fisher4Cast, an open-source, user-friendly Fisher Matrix software with a GUI for automated LATEX file creation and Fisher ellipse generation. Designed for easy extension, it is written in Matlab but portable to Octave and Scilab. Fisher4Cast offers new 3-D and 4-D visualizations of the forecasting landscape and analyzes growth and curvature effects on future cosmological surveys. Available since May 2008 at http://www.cosmology.org.za, it reached 750 downloads in the first year. Version 2.2, released with this paper, includes a Quick Start guide and the code for this paper's figures, benefiting the cosmology and scientific communities.", "First-order logic-based knowledge representation captures the complexity of natural language and supports various probabilistic inference models. While symbolic representation facilitates quantitative reasoning with statistical probability, it poses challenges for integration with machine learning models that rely on numerical operations. Conversely, knowledge embedding\u2014representing knowledge as high-dimensional, continuous vectors\u2014offers a promising approach to complex reasoning, retaining semantic information and establishing quantifiable relationships. In this paper, we introduce the Recursive Neural Knowledge Network (RNKN), which integrates medical knowledge grounded in first-order logic with a recursive neural network to diagnose multiple diseases. By training RNKN on manually annotated Chinese Electronic Medical Records (CEMRs), we develop diagnosis-oriented knowledge embeddings and weight matrices. Our experimental results show that RNKN outperforms traditional machine learning models and the Markov Logic Network (MLN) in diagnostic accuracy. Additionally, the performance of RNKN improves with the extraction of more explicit evidence from CEMRs. As the number of training epochs increases, RNKN progressively elucidates the interpretation of knowledge embeddings.", "In an electron cooler device, multiple solenoids are typically installed to direct the electron beam's motion. These solenoids, however, also affect the ion beam in the cooler's storage ring. If the solenoids in the electron cooler are not perfectly compensated, they can couple the transverse motion of the ion beam in the storage ring. This paper examines the coupled transverse motion resulting from imperfectly compensated solenoids in the CSRm (the main storage ring at the Institute of Modern Physics in Lan Zhou, China). A novel method is used to calculate the coupled beam envelopes.", "We've noticed a near-infrared excess around the white dwarf PHL5038 using UKIDSS photometry, which hints at a cool, substellar companion. To dig deeper, we obtained H- and K-band spectra and images of PHL5038 with the NIRI instrument on Gemini North. Our observations show that the target consists of two distinct components: an 8000K DA white dwarf and what appears to be an L8 brown dwarf companion, which are about 0.94 arcseconds apart. We identified the companion's spectral type using standard indices for late L and T dwarfs. The projected separation between the two is about 55 AU, making this just the second known wide binary system with a white dwarf and a late L dwarf, the first being GD165AB. This particular system could serve as a valuable benchmark for testing substellar evolutionary models, especially for intermediate to older ages.", "We examine how dynamic streams and substructures affect estimates of the local escape speed and the total mass of Milky Way-like galaxies by modeling the high-velocity tail of local halo stars. Utilizing high-resolution, magneto-hydrodynamical cosmological zoom-in simulations, we resolve phase space substructures in regions around solar-like positions. Our findings reveal significant variations in phase space structure between different positions within individual galaxies and across our simulation suite. This substructure unevenly populates the high-velocity tail, causing discrepancies in mass estimates. We demonstrate that streams, sample noise, and truncation of the high-velocity tail below the escape speed contribute to a mass estimate distribution with a median approximately 20% lower than the true value and a spread factor of 2 across the suite. After correcting for these biases, we adjust the Milky Way mass estimate presented by Deason et al. to $1.29^{+0.37}_{-0.47} \\times 10^{12}$ $\\rm M_{\\odot}$.", "By using unconventional techniques, we can extract more than one bit of information per photon through high-dimensional orbital angular momentum (OAM) states for recognizing objects. These OAM states remain unaffected by the object's rotation: the information structure of an object's OAM spectrum stays consistent even if the object rotates randomly between measurements. Furthermore, OAM correlations alone are enough to fully reconstruct images of complex off-axis objects, revealing new object symmetries in the process. We also examine how information rates change when the object shifts off-center in the beam field and find that both the object\u2019s symmetry signals and the information rates remain stable away from the beam center. These findings suggest useful applications for dynamic scanning, especially where symmetry and minimal, noninvasive measurements are needed.", "During Parker Solar Probe's first two trips around the Sun, we noticed a lot of rapid magnetic field flips, called switchbacks. These switchbacks popped up mainly in the near-Sun solar wind, showing up in clusters and possibly connecting to things like magnetic reconnection near the Sun\u2019s surface. Since switchbacks come with faster plasma flows, we wondered if they were hotter than the surrounding plasma and if the tiny details inside a switchback were different from outside it.\n\nWe looked at data from the Solar Probe Cup and focused on times with big changes in angle to compare temperatures inside and outside switchbacks. Our research showed that the reduced distribution functions inside switchbacks matched up with a simple rotation of the background plasma. So, it turns out that the proton core parallel temperature is the same inside and outside switchbacks. This means there's no special temperature-speed relationship for the proton core inside these magnetic field switchbacks.\n\nWe also figured out that switchbacks align with Alfv\u00e9nic pulses moving along open magnetic field lines, but we still don\u2019t know where these pulses come from. Additionally, we saw no clear connection between radial Poynting flux and kinetic energy boosts, meaning that radial Poynting flux doesn\u2019t seem to play a big role in how switchbacks behave.", "Through the Nainital-Cape Survey, we've identified eight $\\delta\\,$Scuti pulsators exhibiting oscillation periods ranging from several minutes to a few hours. To unravel the nature of these observed pulsational variations, we conducted non-adiabatic linear stability analyses on stellar models with masses between 1 and 3 M$_{\\odot}$. Our findings revealed several unstable low order p-modes, and the pulsation periods of these instabilities matched well with the observed data. Notably, for stars HD$\\,$118660, HD$\\,$113878, HD$\\,$102480, HD$\\,$98851, and HD$\\,$25515, we successfully explained the observed variations using low order radial p-mode pulsations.", "A new and detailed examination of classical particle motion within the framework of Lorentz-violating theories is explored. This study employs an enhanced Hamiltonian formalism to derive a Legendre Transformation that bridges the associated covariant Lagrangian and Hamiltonian forms. This methodological advancement facilitates the computation of particle trajectories by utilizing Hamilton's equations in momentum space and the Euler-Lagrange equations in velocity space, effectively bypassing certain singular points inherent in the theory. By imposing conditions that ensure the trajectories are smooth functions of both velocity and momentum, these singularities are naturally resolved. Furthermore, this method allows for the identification of specific segments of the dispersion relations that align with particular Lagrangian solutions. Detailed computations are provided, especially in relation to bipartite Finsler functions. Additionally, a direct connection is established between the derived Lagrangians and the solutions to the Dirac equation in a specific, exemplary case.", "Spectrum management is recognized as a critical step in enabling the technology of cognitive radio networks (CRNs). Most existing research on spectrum management in CRNs focuses on individual aspects of the problem, such as spectrum sensing, spectrum decision, spectrum sharing, or spectrum mobility. In this two-part paper, we propose that, for certain network configurations, performing multiple spectrum management tasks jointly can enhance spectrum efficiency. Our specific aim is to examine the uplink resource management problem in a CRN with multiple cognitive users (CUs) and access points (APs). To maximize their uplink transmission rates, CUs must associate with an appropriate AP (spectrum decision) and share the channels used by this AP with other CUs (spectrum sharing). These tasks are clearly interdependent, and the efficient and distributed execution of these tasks remains an open question in current literature.", "This text presents a simple, solvable statistical model to describe baryonic matter under the thermodynamic conditions of evolving core-collapsing supernovae. The model shows a first-order phase transition in the grand canonical ensemble, which is absent in the canonical ensemble. Like other systems in condensed matter physics, this difference between ensembles is associated with negative susceptibility and jumps in intensive variables related to the order parameter. This behavior arises because baryonic matter experiences both short-range attractive strong forces and long-range repulsive electromagnetic interactions, partly screened by electrons. This is expected in any theoretical treatment of nuclear matter in stars. The study also discusses the implications for supernova dynamics.", "To meet the requirements of a high-performance THz-FEL (Free Electron Laser), a compact FEL injector was proposed. A thermionic cathode, which is simpler and cheaper than a photo-cathode, was chosen to emit electrons. The effective bunch charge was improved to approximately 200pC by using an enhanced EC-ITC (External Cathode Independently Tunable Cells) RF gun to extract micro-bunches, almost eliminating back bombardment effects. Constant gradient accelerator structures were designed to increase energy to around 14MeV, and a focusing system was implemented to suppress emittance and maintain bunch quality. Key components of the FEL injector were analyzed for physical design and beam dynamics. Additionally, start-to-end simulations with multiple pulses were conducted using custom MATLAB and Parmela code. The results indicate that stable, high-brightness electron bunches with low energy spread and emittance can be consistently obtained.", "A whirlwind of claims surrounds the anomalies in the large-angle properties of the cosmic microwave background (CMB) anisotropy measured by WMAP. Yet, the statistical significance of these anomalies remains elusive, primarily because the statistics used to pinpoint these irregularities were selected a posteriori. However, the thrilling possibility of uncovering new physics on the grandest observable scales makes it imperative to scrutinize these claims meticulously. Let's dive into three specific claims: the deficiency of large-angle power, the north-south power asymmetry, and the alignment of multipoles. To tackle the challenge of a posteriori statistics, the solution lies in unearthing a new data set that mirrors the physical scales of the large-angle CMB. Although this is a daunting endeavor, there are promising paths to explore.", "Multi-photon states can be generated through multiple parametric down conversion (PDC) processes. In these processes, a nonlinear crystal is subjected to high-power pumping. Theoretically, the greater the population of these states, the greater the discrepancy with local realistic descriptions. However, high-power pumping often results in low interference contrast in multi-photon PDC experiments. We propose a method to enhance this contrast. Our approach utilizes currently available optical devices, such as multiport beam splitters, which can divide incoming light from a single input mode into $M$ output modes. This method functions as a POVM filter and may enable a viable CHSH-Bell inequality test. Consequently, it could be advantageous in applications such as reducing communication complexity.", "The range of exponents of the transfer matrix determines the localization lengths in Anderson's model for a particle within a lattice subjected to a disordered potential. I demonstrate that combining a duality identity for determinants with Jensen's identity for subharmonic functions yields a formula for the spectrum in terms of the eigenvalues of the Hamiltonian under non-Hermitian boundary conditions. This formula is exact and involves averaging over a Bloch phase instead of disorder. A preliminary study of the non-Hermitian spectra of Anderson's model in dimensions D=1 and D=2, focusing on the smallest exponent, is presented.", "In this article, we enhance extreme learning machines for regression tasks by employing a graph signal processing-based regularization. We presume that the target signal for prediction or regression is a graph signal. Given this assumption, we utilize the regularization to ensure that the output of an extreme learning machine is smooth across a specified graph. Simulation results with real data indicate that this regularization is significantly beneficial, particularly when the available training data is limited in size and affected by noise.", "Get ready for an exciting dive into the heart of turbulent plasmas like the solar wind! When it comes to understanding heating in these weakly collisional systems, we can't ignore inter-particle collisions. These collisions are key players, transforming ordered energy into heat through irreversible relaxation towards thermal equilibrium. Recently, Pezzi et al. (Phys. Rev. Lett., vol. 116, 2016, p. 145001) electrified the field by revealing that plasma collisionality skyrockets thanks to intricate velocity space structures. \n\nBuilding on this groundbreaking discovery, we crank up the energy by comparing how the fully nonlinear Landau operator stacks up against the linearized version. Zeroing in on the relaxation of an out-of-whack distribution function in a homogeneous, force-free plasma, we underscore the crucial role of nonlinearities in the collisional operator for gauging collision effects. Both nonlinear and linearized operators show that different phase space structures dissipate over time. But here\u2019s the thrill: with the linearized operator, these times are consistently longer, indicating that fine velocity structures hang around longer when nonlinearities are omitted. This revelation shakes things up, showing that neglecting nonlinearities means a slower dissipation of these intricate structures. Buckle up as we uncover the dynamic world of collisional effects in turbulent plasmas!", "In this research, we applied Mask-RCNN, a deep-learning algorithm for computer vision and object detection, to semiconductor defect inspection. Stochastic defect detection and classification in semiconductor manufacturing have become increasingly challenging with the continued miniaturization of circuit patterns (e.g., pitches less than 32 nm). Current defect inspection methods using optical and e-beam inspection tools often rely on rule-based techniques, leading to misclassification and requiring human expert intervention. We extended our prior deep learning-based defect classification and detection method to enhance defect instance segmentation in SEM images. This includes accurately defining defect boundaries and generating a mask for each defect category. Our approach allows us to extract, calibrate, and quantify the pixels of each mask, enabling the counting of defect instances and calculation of their surface area. We aim to detect and segment various inter-class stochastic defects, such as bridges, breaks, and line collapses, and accurately differentiate intra-class multi-categorical bridge defects (e.g., thin, single, multi-line, horizontal, non-horizontal) for aggressive pitches and thin resists (High NA applications). Our proposed method demonstrates both quantitative and qualitative effectiveness.", "We present an enhanced and generalized bound on the parameter $\\lambda$ \u2014 the count of common neighbors shared by a pair of adjacent vertices \u2014 in distance-regular graphs $G$. This improvement extends the bounds previously proposed by Spielman (1996) and Pyber (2014) for strongly regular graphs. Our new bound plays a crucial role in recent advancements related to the complexity of testing isomorphism for strongly regular graphs, as discussed by Babai, Chen, Sun, Teng, and Wilmes in 2013. \n\nThe foundation of our proof rests on a clique geometry identified by Metsch in 1991, which under certain parameter constraints, provides significant insights. Additionally, we offer a streamlined proof for an important asymptotic outcome derived from Metsch's work: specifically, if $k\\mu = o(\\lambda^2)$, then every edge in $G$ resides in a unique maximal clique of size approximately equal to $\\lambda$, while all other cliques are of negligible size compared to $\\lambda$. Here, $k$ represents the vertex degree, and $\\mu$ signifies the number of common neighbors between pairs of vertices situated two steps apart.\n\nWe highlight that the cliques identified by Metsch are \"asymptotically Delsarte\" under the condition $k\\mu = o(\\lambda^2)$, thus categorizing families of distance-regular graphs with these parameters as \"asymptotically Delsarte-geometric\".", "Recent galaxy observations reveal that star formation varies dramatically across different galactic environments. To unravel the complexities of galactic-scale star formation, it's essential to understand how giant molecular clouds form and evolve under extreme conditions. Our study zeros in on the puzzling lack of massive stars in the dense molecular gas of strongly barred galaxies. \n\nWe present a detailed hydrodynamical simulation of a strongly barred galaxy, using a stellar potential model based on observational data from NGC 1300. By comparing cloud properties in the bar, bar-end, and spiral arms, we discover that the average cloud virial parameter is consistently around 1, regardless of the environment. This finding rules out the gravitationally-bound state of clouds as the reason for the scarcity of massive stars in strong bars.\n\nInstead, we investigate cloud-cloud collisions, hypothesized to spark massive star formation. Our results show that collision speeds in the bar are significantly higher than in other regions. By analyzing cloud kinetics, we determine that these rapid collisions likely stem from random-like cloud movements caused by the elliptical gas orbits influenced by the bar potential.\n\nThese findings suggest that the observed lack of active star formation in strong bars is due to the high-speed cloud collisions unique to these regions. These fast collisions, driven by violent galactic-scale gas motions, are inefficient at forming massive stars, shedding new light on star formation processes in extreme galactic environments.", "The mass-metallicity relationship (MMR) of star-forming galaxies is well-established, but there are disagreements about its exact shape and its dependence on other factors. \n\nAims: We aim to measure the MMR in the Galaxy And Mass Assembly (GAMA) survey, compare our findings with those from the Sloan Digital Sky Survey (SDSS), and explore how different selection criteria might explain disparities in the literature. \n\nMethods: We use strong emission line ratio diagnostics to determine oxygen abundances and apply various selection criteria, including minimum signal-to-noise in emission lines and apparent and absolute magnitude, to study variations in the inferred MMR.\n\nResults: The shape and position of the MMR can vary significantly depending on the metallicity calibration and selection criteria used. After choosing a robust metallicity calibration, we find that the MMR for redshifts 0.061 < z < 0.35 in GAMA aligns reasonably well with that in the SDSS, despite differences in the luminosity range.\n\nConclusions: Given the significant variations in the MMR due to changes in sample selection criteria and methods, we recommend caution when directly comparing MMR results from different surveys. Additionally, there may be modest evolutionary changes in the MMR within the GAMA sample over the redshift range of 0.06 < z < 0.35.", "Through thermodynamic analysis, we develop a set of equations that describe the seepage velocities of fluid components in immiscible and incompressible two-phase flow within porous media. These equations introduce a novel velocity function known as the co-moving velocity, which is intrinsic to the characteristics of the porous medium. In conjunction with a constitutive relation linking velocities to driving forces, such as the pressure gradient, these equations form a comprehensive framework. We apply this theoretical framework to analytically solve four variants of the capillary tube model and validate the theory numerically using a network model.", "Nature's inventiveness, shown in the diversity of life forms and functions, sets living beings apart from nonliving entities. It's no surprise that this trait is a central focus in artificial life studies. Since Darwin, we've known that this diversity arises from evolution, termed Open-Ended Evolution (OEE) in the field. This article is the second of two special issues on current OEE research, summarizing the contents of both. The work featured here was primarily presented at the 2018 Conference on Artificial Life in Tokyo, with roots in prior workshops held in Cancun and York. We provide a simplified categorization of OEE and summarize the progress reflected in the articles within this special issue.", "Discover the dynamic properties of MgO/Ag(001) ultrathin films with substitutional Mg atoms integrated into the interface metal layer! Through a combination of Auger electron diffraction experiments, ultraviolet photoemission spectroscopy, and density functional theory (DFT) calculations, we delve deep into the subject.\n\nUtilizing the layer-by-layer resolution of the Mg KL_23 L_23 Auger spectra and advanced multiple scattering calculations, we begin by pinpointing the interlayer distances and key morphological parameters of the MgO/Ag(001) system, both with and without Mg integration at the interface. Our findings reveal that incorporating Mg atoms induces a significant distortion in the interface layers, profoundly affecting the metal/oxide electronic structure. Notably, there\u2019s a substantial reduction in the work function by 0.5 eV, attributed to variations in the band offset at the interface.\n\nOur experimental observations align perfectly with DFT calculations, which not only replicate the lattice distortion but also, through a Bader analysis, uncover that increasing the interface Mg concentration causes electron transfer from Mg to Ag atoms within the metallic interface layer. This local lattice distortion emerges due to the attractive and repulsive Coulomb interactions between O2- ions in the MgO interface layer and the positively (negatively) charged Mg (Ag) neighbors.\n\nInterestingly, although this distortion impacts the work function, the reduction effect is somewhat limited. Ultimately, our analysis of the induced work function changes\u2014examining charge transfer, rumpling, and electrostatic compression\u2014concludes that the primary driver behind the metal/oxide work function alterations due to interface Mg atom incorporation is the increase in electrostatic compression.", "Keeping a close eye on industrial processes is crucial. Spotting changes in process parameters quickly lets you fix problems before they get out of hand. This task gets particularly tricky when your measurements fall below the sensitivity limits of your system or detection limits, making your observations incomplete - these are known as left-censored data. When more than 70% of your data faces this issue, traditional methods won't cut it. You need special statistical techniques to get a true picture of your process's health. This paper introduces a method to estimate process parameters in these challenging situations and unveils a corresponding control chart, complete with a detailed algorithm.", "Clustering plays a vital role in various data-driven fields and has been heavily researched with respect to distance metrics and clustering algorithms. However, there has been limited attention given to the learning of representations specifically for clustering purposes. In this study, we introduce Deep Embedded Clustering (DEC), an approach that concurrently learns both feature representations and cluster assignments through the use of deep neural networks. DEC creates a mapping from the original data space to a reduced-dimensional feature space, continuously refining a clustering objective. Our experiments with image and text datasets demonstrate a notable enhancement compared to the current leading techniques.", "We extend the insightful research by Sarvotham et al. [2005], which demonstrated the impact of peak transmission rates on network burstiness. By analyzing TCP packet headers, we group packets into sessions, each defined by a 5-tuple: (S, D, R, Peak R, Initiation T) = (total payload, duration, average transmission rate, peak transmission rate, initiation time). After thorough analysis, we propose a new definition for peak transmission rates. Contrary to Sarvotham et al. [2005], who divided sessions into two categories\u2014alpha and beta\u2014we distribute them into ten segments based on the empirical quantiles of the peak rate variable. This approach showcases that the beta group is not uniform. Our refined segmentation uncovers additional structural details missed by the previous two-group method. In each segment, we examine the dependence structure of (S, D, R) and observe that it varies significantly across different groups. Moreover, within each segment, session initiation times closely follow a Poisson process, a property not observed when evaluating the dataset as a whole. Consequently, we conclude that peak rate levels are critical for understanding network structure and for creating accurate simulations of real-world data. Based on our findings, we propose a straightforward method for simulating network traffic.", "The Brouwer fixed-point theorem in topology asserts that any continuous function mapping a compact convex set into itself has at least one fixed point, that is, a point \\( x_0 \\) such that \\( f(x_0) = x_0 \\). Under specific conditions, this fixed point can be associated with the throat of a traversable wormhole, represented by \\( b(r_0) = r_0 \\) for the shape function \\( b = b(r) \\). Consequently, the potential existence of wormholes can be inferred from purely mathematical principles without exceeding current physical constraints.", "Convolutional Neural Networks (CNNs) have recently been utilized to address problems in both computer vision and medical image analysis. However, most methods can only handle 2D images, while the majority of medical data used in clinical settings consists of 3D volumes. In this study, we present a method for 3D image segmentation using a volumetric, fully convolutional neural network. Our CNN is trained end-to-end on MRI volumes of the prostate, allowing it to predict segmentation for the entire volume simultaneously. We introduce a new objective function based on the Dice coefficient, optimized during training, which helps manage the imbalance between foreground and background voxels. To counter the limited number of annotated volumes for training, we enhance the data by applying random non-linear transformations and histogram matching. Our experimental results demonstrate that our method performs well on challenging test data and significantly reduces processing time compared to previous methods.", "The non-relativistic two-body system with Coulomb interaction has a spectrum known as the Balmer series, $E_n=\\frac{\\alpha^2m}{4n^2}$, derived from the Schr\u00f6dinger equation. In 1954, Wick and Cutkosky found that for $\\alpha>\\frac{\\pi}{4}$, relativistic effects in the Bethe-Salpeter equation result in additional levels beyond the Balmer series. The nature of these new states was unclear and questioned. Recently, we have shown that these extra states are dominated by the exchange of massless particles moving at the speed of light, which is why they are absent in the non-relativistic Schr\u00f6dinger framework.", "We investigate the fundamental characteristics of quantum f-relative entropy using f(.) as an operator convex function. We establish the conditions for equality under monotonicity and joint convexity, which are more comprehensive than previous conditions as they apply to a broader class of operator convex functions and differ from those for f(t) = -ln(t). Additionally, we define quantum f-entropy based on quantum f-relative entropy, exploring its properties and presenting the equality conditions in certain scenarios. Furthermore, we demonstrate that the f-extended versions of Holevo information, the entanglement-assisted capacity, and the coherent information adhere to the data processing inequality, and we provide the equality conditions for the f-coherent information.", "Modifying or shifting an input image should not alter the outcomes of various computer vision tasks. Convolutional neural networks (CNNs) inherently maintain translation equivariance; hence, translating an input image results in correspondingly translated feature maps. However, this property is not shared with rotations. While data augmentation is commonly used to achieve global rotation equivariance, maintaining equivariance at the patch level is more challenging. We introduce Harmonic Networks, or H-Nets, which are CNNs that ensure equivariance to both patch-wise translation and 360-degree rotation. This is accomplished by substituting standard CNN filters with circular harmonics that provide a maximal response and orientation for each receptive field patch. H-Nets offer a rich representation that is both parameter-efficient and computationally inexpensive. We demonstrate that deep feature maps within the network inherently encode intricate rotational invariants. Our layers are versatile and compatible with recent architectural advances and methods, such as deep supervision and batch normalization. Additionally, we achieve state-of-the-art classification performance on rotated-MNIST and deliver competitive results on other benchmark datasets.", "We analyze reflection spectra of directly coupled waveguide and cavity systems. The observed Fano lines reveal insights into reflection and coupling processes. Unlike side-coupled systems, the Fano line shape stems from the coupling process between the measurement device fiber and the waveguide, not the waveguide termini. Our experimental results and analytical model show that the Fano parameter is highly sensitive to the coupling condition. Even minimal movement of the fiber, below the Rayleigh range, can drastically change the Fano line shape.", "The strength and vertical distribution of atmospheric turbulence are crucial factors affecting the performance of optical and infrared telescopes, both with and without adaptive optics. However, measuring these parameters remains challenging. We introduce a novel technique that utilizes a sequence of short-exposure images of a star field taken with a small telescope. By analyzing the differential motion between all pairs of star images, we compute the structure functions of longitudinal and transverse wavefront tilt across various angular separations. These measurements are then matched with theoretical predictions from simple turbulence models using a Markov-Chain Monte-Carlo optimization process. This approach allows us to estimate the turbulence profile in the lower atmosphere, determine the total and free-atmosphere seeing, and assess the outer scale. We present results from Monte-Carlo simulations that validate the technique and provide examples using data from the second AST3 telescope at Dome A in Antarctica.", "We're excited to introduce the concept of an n-plectic structure! It's a dynamic combination of a commutative and torsionless Lie Rinehart pair along with a unique cocycle from its Chevalley-Eilenberg complex. This 'n-plectic cocycle' sparks an extension of the Chevalley-Eilenberg complex with what we call symplectic tensors. The cohomology of this extension brings a thrilling expansion to Hamiltonian functions and vector fields, transforming them into tensors and cotensors across various degrees, up to certain coboundaries, and forming a robust Lie oo-algebra structure. And that's not all\u2014momentum maps burst onto the scene here as compelling weak Lie oo-morphisms from any Lie oo-algebra into the vibrant Lie oo-algebra of Hamiltonian (co)tensors. Get ready to dive into this exhilarating new realm!", "Amorphous solids or glasses often show stretched-exponential decay in their various properties over long periods, like in the intermediate scattering function, dielectric relaxation modulus, and time-elastic modulus. This is especially noticeable near the glass transition. In this Letter, we focus on dielectric relaxation to demonstrate that this kind of stretched-exponential relaxation is closely linked to the unique lattice dynamics of glasses. By reworking the Lorentz model of dielectric matter in a more general way, we can describe the dielectric response based on the vibrational density of states (DOS) for a random collection of spherical particles that interact harmonically with their nearest neighbors. Interestingly, we discovered that near the glass transition (which aligns with the Maxwell rigidity transition) this system's dielectric relaxation matches stretched-exponential behavior with Kohlrausch exponents between 0.56 and 0.65, which is consistent with experimental observations. The key reason for stretched-exponential relaxation can be traced back to soft modes, known as the boson peak, in the DOS.", "To demonstrate the challenges of achieving representation disentanglement in the text domain without supervision, we selected successful models from the image domain. We evaluated these models using six disentanglement metrics, downstream classification tasks, and homotopy. To aid in evaluation, we created two synthetic datasets with known generative factors. Our experiments reveal a gap in the text domain and show that elements like representation sparsity or coupling with the decoder can affect disentanglement. To our knowledge, this is the first work addressing unsupervised representation disentanglement in text, providing a framework and datasets for future research.", "This paper introduces a hybrid quantum-classical algorithm to address the unit commitment (UC) problem in power systems. The UC problem is broken down into three components: a quadratic subproblem, a quadratic unconstrained binary optimization (QUBO) subproblem, and an unconstrained quadratic subproblem. The first and third subproblems are solved using a classical optimization solver, while the QUBO subproblem is tackled with a quantum algorithm known as the quantum approximate optimization algorithm (QAOA). These subproblems are then iteratively coordinated through a three-block alternating direction method of multipliers algorithm. Using Qiskit on the IBM Q system for simulations, the results confirm the effectiveness of the proposed algorithm in solving the UC problem.", "It has recently been postulated that the identification of numerous extremely low-amplitude oscillatory modes within Delta Scuti stars hinges upon achieving a high signal-to-noise ratio. Such a significant discovery, unattainable through ground-based telescopes, represents a primary scientific objective of the CoRoT (Convection Rotation and planetary Transits) space mission, which is developed and operated by CNES (Centre National d'\u00c9tudes Spatiales). This paper delineates the findings derived from the study of HD 50844, where an extensive dataset comprising 140,016 data points underwent analysis via independent methodologies with multiple verifications performed. Remarkably, a precision of \\(10^{-5}\\) magnitude was achieved in the amplitude spectra of the CoRoT time series.\n\nThe frequency analysis of the CoRoT data unveiled an extensive array of frequencies, numbering in the hundreds, within the range of 0\u201330 d\\(^{-1}\\). Every cross-validation technique confirmed this groundbreaking observation. Consequently, the initial hypothesis that Delta Scuti stars possess a very rich and intricate frequency structure stands substantiated. Furthermore, spectroscopic mode identification corroborated this by recognizing modes of very high degrees, up to \\(\\ell = 14\\). This study also demonstrates that the anticipated cancellation effects are inadequate in eliminating the flux variations pertaining to these modes at the precision level provided by CoRoT measurements.\n\nGround-based observations have indicated that HD 50844 is an evolved star, slightly deficient in heavy elements, currently positioned at the Terminal Age Main Sequence. Owing possibly to this particular evolutionary stage, the frequency set does not exhibit a clear regular distribution. The dominant frequency component (\\(f_1 = 6.92\\) d\\(^{-1}\\)) has been identified as the fundamental radial mode, a determination made through the combined use of photometric and spectroscopic data from ground-based sources.\n\nAdditionally, this research is underpinned by observations made utilizing ESO (European Southern Observatory) telescopes as part of the ESO Large Programme LP178.D-0361. It also incorporates data gathered from the Observatorio de Sierra Nevada, the Observatorio Astron\u00f3mico Nacional at San Pedro M\u00e1rtir, and the Piszk\u00e9stet\u0151 Mountain Station of Konkoly Observatory.", "The article dives into the fascinating observations of star-forming regions S231-S235 through 'quasi-thermal' lines of ammonia (NH$_3$) and cyanoacetylene (HC$_3$N), as well as the maser lines of methanol (CH$_3$OH) and water vapor (H$_2$O). These regions are part of the giant molecular cloud G174+2.5. By analyzing archival CO data, we identified all the massive molecular clumps in G174+2.5. For each of these clumps, we determined mass, size, and CO column density before conducting detailed observations.\n\nExcitingly, we report the first detections of NH$_3$ and HC$_3$N lines in the molecular clumps WB89 673 and WB89 668, indicating the presence of high-density gas. Using ammonia emission data, we estimated the physical properties of the molecular gas within these clumps. We discovered that the gas temperature ranges from 16 to 30 K and the hydrogen number density spans from 2.8 to 7.2$\\times10^3$ cm$^{-3}$. Adding to the intrigue, we newly detected the shock-tracing line of the CH$_3$OH molecule at 36.2 GHz toward WB89 673.", "We present the lowest frequency observations of gamma-ray burst (GRB) 171205A using the upgraded Giant Metrewave Radio Telescope (uGMRT), spanning a frequency range of 250\u20131450 MHz and monitored over a period of $4-937$ days. This is the first GRB afterglow detected within the 250\u2013500 MHz range and the second brightest GRB observed with the uGMRT. Despite observing the GRB for nearly 1000 days, there is no sign of it entering a non-relativistic phase. We also examine archival ${\\it Chandra}$ X-ray data from approximately day 70 and day 200 but find no evidence of a jet break from the combined data analysis. We model the synchrotron afterglow emission originating from a relativistic, isotropic, self-similar deceleration and from a shock-breakout of a wide-angle cocoon. Our data enables us to analyze the nature and density of the surrounding medium of the burst. The results indicate a deviation from a standard constant density medium, suggesting the GRB occurred in a stratified wind-like environment. Our findings show that the lowest frequency measurements, which cover the absorbed sections of the light curves, are essential to uncover the GRB environment. When combined with other published data, our observations suggest that the radio afterglow includes contributions from two components: a weak, perhaps slightly off-axis jet, and a surrounding wider cocoon, matching the results of Izzo et al. (2019). The cocoon emission likely dominates during early epochs, while the jet becomes more significant at later times, leading to flatter radio light curves.", "The recently developed theory of quasi-Lie schemes is applied to various Emden-type equations. A method to handle these and their generalizations is presented. We derive t-dependent constants of motion for specific Emden equations using particular solutions, recovering previously known results. Additionally, t-dependent constants of motion for certain Emden-type equations are also recovered.", "We explore the study of charged Higgs bosons as predicted by a model characterized by the gauge symmetry $SU(3)_c \\otimes SU(3)_L \\otimes U(1)_X$. By examining Yukawa couplings that mix scales from small (around GeV) to large (around TeV), we demonstrate that the model foresees the production of hypercharge-one $H_1^{\\pm}$ and hypercharge-two $H_2^{\\pm}$ Higgs bosons in $pp$ collisions, though at varying production rates. At lower energies, $H_1^{\\pm}$ bosons behave similarly to the charged Higgs bosons found in a two Higgs doublet model (2HDM), while $H_2^{\\pm}$ bosons are additional like-charged Higgs particles originating from the 3-3-1 framework. Consequently, identifying multiple like-charged Higgs boson resonances could validate the alignment between theoretical models and experimental results. We investigate the production of $H_{1,2}^{\\pm}$ pairs and their associated production with $tb$ at the CERN LHC collider. Notably, our findings suggest that pair production in gluon-gluon collisions can be as prominent as single production, facilitated by the exchange of a heavy neutral $Z'$ gauge boson proposed by the model. When considering the decays into leptons $H_{1,2}^{\\pm} \\rightarrow \\tau \\nu_{\\tau}$, we identify scenarios where small peaks of $H_{2}^{\\pm}$-boson events in transverse mass distributions stand out against the $H_{1}^{\\pm}$ background.", "Isospin breaking in the $K_{\\ell 4}$ form factors, caused by the difference between charged and neutral pion masses, is examined using a framework based on suitably subtracted dispersion representations. These form factors are iteratively constructed up to two loops in the low-energy expansion, ensuring analyticity, crossing symmetry, and unitarity from two-meson intermediate states. The paper provides analytical expressions for the phases of the two-loop form factors in the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ channel, enabling the connection between experimentally measured form-factor phase shifts (beyond the isospin limit) and theoretically studied $S$- and $P$-wave $\\pi\\pi$ phase shifts (within the isospin limit). Unlike previous analyses based on one-loop chiral perturbation theory, this study explores the dependence on the two $S$-wave scattering lengths $a_0^0$ and $a_0^2$ in a comprehensive manner. The phases of the $K^\\pm\\to\\pi^+\\pi^- e^\\pm \\nu_e$ form factors, as obtained by the NA48/2 collaboration at CERN SPS, are reanalyzed, incorporating isospin-breaking corrections to extract precise values for the scattering lengths $a_0^0$ and $a_0^2$.", "In this exciting paper, we take the groundbreaking results from \\cite{1,2,3,4}, especially \\cite{4}, and push them even further to deliver a thrilling statistical description of the cosmological constant in a cosmological de Sitter universe through the lens of massless excitations with Planckian effects! To start, we reveal that at a classical level, a positive cosmological constant $\\Lambda>0$ is achievable only as $T\\rightarrow 0$. Drawing a fascinating parallel to black holes, we show that once quantum effects are factored in, $\\Lambda$ can indeed be represented via massless excitations, thanks to the inclusion of quantum corrections to the Misner-Sharp mass. But there's more! Quantum fluctuations introduce an effective cosmological constant that varies with the physical scale considered, presenting a potential breakthrough solution to the cosmological constant problem without needing a quintessence field. The minuscule actual value of $\\Lambda$ could be attributed to a quantum decoherence scale above the Planck length, allowing spacetime to evolve as a pristine de Sitter universe with a small averaged cosmological constant, beautifully frozen in its lowest energy state. How captivating is that?", "We explore the properties of a one-dimensional spin-glass model with vector spins at both zero and finite temperatures, focusing on the scenario where the number of spin components, m, approaches infinity, and the interactions decay with a power, \\(\\sigma\\), relative to distance. Additionally, a diluted version of the model is examined, revealing significant differences compared to the fully connected model. At zero temperature, we calculate defect energies by comparing the ground-state energies of systems with periodic and antiperiodic boundary conditions, establishing the relationship between the defect-energy exponent \\(\\theta\\) and \\(\\sigma\\). The relationship \\(\\theta = 3/4 - \\sigma\\) provides a solid fit, indicating the upper critical value of \\(\\sigma\\) is 3/4, which aligns with the lower critical dimension for the d-dimensional short-range version of the model. For finite temperatures, we solve the large m saddle-point equations self-consistently, allowing us to access the correlation function, order parameter, and spin-glass susceptibility. We pay particular attention to the different forms of finite-size scaling effects below and above the critical value \\(\\sigma = 5/8\\), corresponding to the upper critical dimension 8 for the hypercubic short-range model.", "Of^+ supergiants have properties between regular O-stars and Wolf-Rayet (WR) stars, with significant similarities to WN-type objects, especially in the visible and near-infrared spectra, indicating common stellar wind properties. We present the first dedicated X-ray observations of HD16691 (O4If^+) and HD14947 (O5f^+). These observations reveal a soft thermal spectrum consistent with single O-type star emissions but with slightly lower X-ray luminosity than expected. This suggests their stellar wind properties significantly impact X-ray emission, marking an intermediate stage between O and WR stars due to enhanced wind density.", "The AARTFAAC project is set to revolutionize astronomical observation by implementing an All-Sky Monitor (ASM) powered by the Low Frequency Array (LOFAR) telescope. This cutting-edge initiative will enable real-time, 24/7 monitoring of low-frequency radio transients across the majority of the sky visible to LOFAR, capturing events in timescales from milliseconds to several days. A crucial aspect is its capability for rapid follow-up observations with the full LOFAR upon detecting potential transient candidates.\n\nThe project's ambitious goals present numerous implementation challenges: imaging the vast all-sky field, ensuring low-latency processing, maintaining continuous availability, and achieving autonomous operation of the ASM. The first challenge has already led to the development of the world's largest correlator for the ASM, boasting an unprecedented number of input channels. Once operational, it will produce about 150,000 correlations per second per spectral channel.\n\nInitial test observations using the existing LOFAR infrastructure have been conducted to fine-tune and validate the ASM's design parameters. This paper provides a comprehensive overview of the AARTFAAC data processing pipeline and highlights some of the core challenges by showcasing all-sky images from a test observation. These preliminary results offer quantitative insights into the instrument\u2019s potential capabilities.", "Wolf-Rayet (WR) stars are evolved successors of massive O-type stars and are believed to be potential precursors to Type Ib/c core-collapse supernovae (SNe). Our recent HST/WFC3 survey of Wolf-Rayet stars in M101 highlights findings based on the detection efficiency of narrow-band versus broad-band optical imaging methods. We demonstrate that, on average, 42% of WR stars, rising to approximately 85% in central regions, are exclusively detected using narrow-band imaging. Consequently, the absence of WR stars at the locations of around 10 Type Ib/c SNe in broad-band imaging no longer strongly suggests a non-WR progenitor pathway."]